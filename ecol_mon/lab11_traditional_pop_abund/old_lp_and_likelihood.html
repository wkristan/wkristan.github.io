<!DOCTYPE html>
<html>
  <head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
    <title>Pop size 1</title>
    <link href="https://csusm-my.sharepoint.com/personal/wkristan_csusm_edu/_layouts/15/guestaccess.aspx?docid=13bc7dccaf86d43ebbe7abfb2e79cc384&authkey=AcDadEBxh8k5g7b9-40JXAo"
      rel="stylesheet" type="text/css">
    <script src="https://csusm-my.sharepoint.com/personal/wkristan_csusm_edu/_layouts/15/guestaccess.aspx?docid=179808882d4bd4f738c6eca25d8c4127d&authkey=AQ2PMP1h8P6fGbdSyA4-cxw"></script>
  </head>
  <body>
    <h1>Estimating population size</h1>
    <p>Population size is a basic piece of information needed for monitoring
      species of concern. Counting numbers of individuals in a species is a
      necessary, but usually not sufficient, step in assessing the size of a
      population. Even species that are big, stationary, and obvious may be too
      numerous to count for logistical reasons. In most instances, we may not
      actually need to have a complete count of individuals (called a "census"),
      and a reliable estimate of population size is more than enough.</p>
    <p>A reliable estimate is one that we are sufficiently confident is correct
      that we can use it to address our management needs. For example, consider
      a population that has been estimated to have 1000 ± 200 individuals in it.
      We won't be able to detect a decline until the population size has
      declined enough to be outside of the confidence intervals, which is more
      than a 20% decline. Whatever methods we use have to not only give us
      accurate (a.k.a. unbiased) estimates, they have to give us estimates that
      are sufficiently precise that we can use them to guide our management
      decisions.</p>
    <p>As we saw in lecture, estimating abundance of sessile organisms, like
      plants and some animals, can be done with simple extrapolations from mean
      numbers in quadrats. There are potential problems with this approach, in
      that numbers counted at one scale don't always extrapolate linearly to
      another scale and not all individuals may be countable at the particular
      time you do your sampling. But, we will treat this as a relatively simple
      problem, and will focus in this lab on the thornier issues we face with
      estimating populations of animals.</p>
    <h2>Estimating animal abundance</h2>
    <p>We have long understood that we cannot expect to see every organism that
      is present at the time we count them. Methods to deal with this have been
      in development since the 1930's, and there are now a wide variety of
      techniques available to deal with all kinds of data collection approaches
      and study designs.</p>
    <h3>Methods that do not require marking</h3>
    <p><img style="width: 370px; height: 288px;" alt="CPUE method" src="icr_rw_kasa_f1.gif"
        align="left" hspace="5">Some types of estimators, called "removal"
      estimators, trap and remove (usually lethally) animals from an area over
      time, and then plot the catch per unit effort against the cumulative
      catch. This "Leslie plot" to the left, which depicts depletion of a
      species of whales during whaling, estimates the total population size as
      the x-intercept of the line fit through the data - this is, the catch per
      unit effort will go to zero when the cumulative catch equals the
      population size, because all the whales would be gone at that point. This
      type of estimate works well and has the advantage that even though many
      animals are never seen, it isn't necessary to mark and recapture any of
      the animals in the population to use it. On the other hand, it has the
      distinct disadvantage that it reduces the population in order to estimate
      its size (you would need to subtract the cumulative catch from the
      estimate of the initial size, since it would no longer be at the initial
      size). This is an unacceptable drawback when one is trying to assess the
      status of a population without altering it.</p>
    <p>However, removal methods are popular in harvested populations, in which
      the removal is going to happen anyway, and is not being done just to
      estimate the size of the population. That the method works without needing
      to trap and mark the animals in advance is beneficial in those cases.</p>
    <p>One way we could use this general approach without having to remove
      animals is that we could capture them, mark them with an identifiable tag,
      and release them unharmed. Assuming that the marked animals don't
      interfere with capture of unmarked animals later, then each time we
      capture a new batch we could ignore the animals that are already marked,
      and just count the unmarked ones as new captures - they would then be
      marked and released unharmed as well. If we plotted the cumulative catch
      of new, unmarked individuals on the x-axis, and CPUE of new, unmarked
      individuals on the y-axis, we would be able to estimate the population
      size without having to remove animals. </p>
    <p>In general, any method that estimates population size without removing
      animals is going to require marking some of the individuals in the
      population.</p>
    <h3>Methods that require marking</h3>
    <p>Most techniques for estimating population size (and survival probability
      as well, as we will learn in a couple of weeks) require that some subset
      of the population is marked. Marks come in a wide variety of types, and
      include leg bands and "petagial" tags (which attach to the wings) for
      birds, "<a href="floy.jpg">floy</a>" tags for fish, ear tags and neck
      collars for mammals, and very <a href="bee%20on%20finger.jpg">tiny</a>
      glue-on labels for bees. In some cases it is possible to use natural
      markings on animals to identify them, as has been done successfully with <a
        href="predator2.jpg">killer whales</a>, and with whisker spot patterns
      and scarring for <a href="Whiskerspotimagecompressed.jpg">lions</a> in
      Africa. The most informative type of marking is unique to individuals, so
      that it's possible to know not only that the individual had been captured
      once before when it is seen again, but also to construct an entire capture
      history for the individual. The likelihood-based technique we will learn
      today requires individual identification, but the Lincoln-Petersen method
      only requires "batch" marks that don't have to be unique to individuals
      (although, of course, you can use marks that are unique to individuals and
      just ignore the individual identifiers when you use the estimator, which
      is what we will do today).</p>
    <h3>The Lincoln-Petersen method</h3>
    <p>The Lincoln-Petersen method is one of the oldest mark-recapture methods
      available. LP employs two capture periods. In the first period, every
      individual captured is marked and released. In the second period a sample
      of individuals is taken, and the number of marked individuals, and total
      number of individuals are recorded. This is a "batch mark" method, in that
      we don't need to know the animals individually. It is also a "closed
      population" method, in that we assume both demographic closure (i.e. no
      births or deaths) and geographic closure (i.e. no immigration or
      emigration). We do not need to capture all of the individuals present in
      the first capture period, and we do not need to re-capture all of the
      marked individuals in the second. We will use the ratio of marked
      individuals to total individuals in the second sample (which is a known
      quantity) along with the total number of marked individuals in the
      population (which is known, since we marked and released them) to estimate
      the population size.</p>
    <p>We will be working with some (instructor generated) data that mimics a
      mark/recapture study, as might be done on desert tortoises. The study
      started on 4/21/13. There were three different capture periods, one in
      April, one in May, and one in June. Each animal caught was marked, and
      dates of recaptures were recorded. The marks were individual and
      permanent. </p>
    <p>The duration of the study was short enough, and during a time or year in
      which mortality is low enough for this species, that we can consider the
      population to be closed demographically and geographically closed. </p>
    <p>We will work with the April and May data for today (we will work with all
      three capture periods next week). </p>
    <p>So, on to the LP estimator.</p>
    <p>1. <strong>Download the data</strong>. <a href="capture_data.xls">This
        file</a> has the data we will work with. If you open it in Excel, you'll
      see that it has only two columns. Column A has dates of captures, and
      column B has tag ID's for the captured animal. In a usual mark/recapture
      study there would be a lot more data than this, because capturing animals
      is difficult enough (and risky enough to the animals) that you would
      record as much information about it as possible once you got your hands on
      it. We would usually have information about the sex, age, perhaps some
      morphometric information (weight, lengths, etc.), ID's of any samples
      taken, and the like. But, for population size estimation all we need to
      know is the date of each capture.</p>
    <p>You can get a better idea of what the data look like if you "filter" on
      animal ID's. Click into the column heading for one of the columns, and
      click on the "Sort &amp; Filter" button to the right of the "Home" tab,
      and select "Filter". You'll see you now have drop-down arrows for each of
      the columns. If you click on "Date", you can select one of the months to
      display by un-checking the other two, or you can select just one of the
      days. For example, display only 4/21/2013 by un-checking May and June, and
      then un-checking 22 and 23 in April. You'll see that there were 24 animals
      that were caught initially on April 21, and that their ID's are assigned
      sequentially as A1-A24. Now if you add the 22nd, you'll see that some of
      the animals caught on the 21st were recaptured on the 22nd (that is, there
      are ID's on 4/22 that are in the range A1-A24), and an additional 21
      animals were added to the marked population on the 22nd. You can "Select
      all" to go back to displaying all the data. </p>
    <p>Now, filter based on Tag ID. If you pick the first animal caught, A1,
      you'll see it was only captured once, on the first day. If you select the
      next one (which is A10, the next one in an alphabetical sort rather than a
      numeric one) you'll see that animal was caught twice, once on the first
      day and once on 5/20/13. As you saw at the beginning, though, some animals
      were re-captured on the second day of the April trapping period, and this
      is a complication for us - we want to build capture histories that have
      nothing but 0's and 1's, one for each trapping period (month), so we will
      need to use a method of summarizing the data that will not result in
      anything other than 0's and 1's. Fortunately, we have a way.</p>
    <p>2. <strong>Add a column for captur</strong><strong>e period and for ones</strong>.
      For the LP estimator we need to designate a "Mark" and a "Recapture"
      period. If you look at dates, you'll see that the trapping took place over
      three days each month. Since we're fortunate enough to have the three
      sample periods in three different months, we can easily identify the
      periods by month.</p>
    <p>You can turn off the filtering by selecting the "Sort &amp; Filter" -&gt;
      "Filter" again.</p>
    <p>In cell C1 type "Month", and in C2 type =month(a2). Copy and paste this
      to the rest of the rows.</p>
    <p>Now, in cell D1 type "Ones", and in cell D2 type the number 1. Copy and
      paste this to the rest of the rows, so that you a single 1 for each row of
      data. You'll see why this is needed in the next step.</p>
    <p>3. <strong>Make a pi</strong><strong>v</strong><strong>ot table</strong>.
      Select a cell within the data set, and select "Insert" -&gt; "Pivot
      table". The outline of the data will be found, and the blank pivot table
      template will be started in a new sheet. We want a table that will tell us
      whether an individual was detected in each of the months, so put Tag ID in
      the row labels, Month in as column labels, and Ones as the data field.</p>
    <p>Change the value field to display the maximum (or&nbsp; instead of a sum.
      For animals that were reported more than once in a month, this will cause
      just a single 1 to be reported. </p>
    <p>When you finish the PivotTable, you'll see that the rows of the table
      look a bit like a capture history, except that there are blanks instead of
      0's in the table. If you right-click into the table and select "PivotTable
      Options", enter a 0 in the "For empty cells show:" field, and click OK.</p>
    <p>This gives us a table that displays the capture history for each
      individual, but that's not what we need. To estimate population size we
      need to know the frequency of each capture history across the individuals
      in the data set. We'll get those next.</p>
    <p>4. <strong>Make capture histories and count frequencies</strong>. First,
      to make this a little easier for today, we'll filter out the June
      captures. Click on "Column Labels" and un-check 6. Now you will only have
      captures for April and May reported.</p>
    Next, we need to make a new column outside of the PivotTable that has the
    capture histories, including the 0's for the non-captures. In cell G4 type
    the name "Histories". In G5 type =b5&amp;c5. The ampersand is the
    "concatenation" operator, and it will stick the two values together into
    one. Copy and paste this to the rest of the rows.
    <p>Now, click on "Histories" and insert a new pivot table. Put the new pivot
      table into the same sheet as the histories - you'll see an option to
      "Choose where you want the PivotTable report to be placed. Pick "Existing
      Worksheet", and select cell I1 for the table. You will only have one
      variable, Histories, to work with. Put it into the row labels and the
      value field, and change the value field to a count. You now have a pivot
      table giving the histories and their frequencies.</p>
    <p>5. <strong>Calculate the values needed for LP</strong>. For the LP
      estimator, we need to know:</p>
    <p>M = total marked individuals in the first sample (April)</p>
    <p>c = total animals caught in the second sample (May)</p>
    <p>r = animals recaptured in the second sample.</p>
    <p>In cell I11 type "M". M includes every individuals caught in April,
      because all of them were marked when they were caught. The two histories
      that correspond with capture in April are 10 and 11. So, in cell J11 add
      these two frequencies.</p>
    <p>(Note that if you are trying to build formulas by clicking into cells of
      the PivotTable you'll probably get an odd "GetPivotData" reference
      inserted. You can turn off this undesirable behavior by selecting "File"
      -&gt; "Options", the in the "Formulas" options un-check the "Use
      GetPivotData functions..." option.)</p>
    <p>Next, in I12 type "c". Animals that were captured in May are histories 01
      and 11. In cell J12 sum the frequencies of these two histories.</p>
    <p>Finally, in cell I13 type "r". The recaptured animals are the 11
      histories. Enter the frequency of 11's in cell J13.</p>
    5. <strong>Calculate population size</strong>. Estimated population size is
    just N = Mc/r. In cell I15 type "N-hat", and in J15 calculate the estimate.
    <p>6. <strong>Calculate the 95% confidence interval</strong>. Now that we
      have an estimate, we need to also calculate the 95% confidence interval
      for it.</p>
    <p>In cell I16 type "Var(N-hat)" - the variance of an estimator is the
      square of the standard error, and we need the standard error for our CI.
      The formula for the variance of N-hat is:</p>
    <p>((M+1)*(c+1)*(M-r)*(C-r))/(((r+1)^2)*(r+2))</p>
    <p>I'll leave it to you to translate substitute the needed cell references
      and enter the formula in cell J16 (if you get it right the value will be
      1824).</p>
    <p>Next, in cell I17 type "se", and in J17 take the square root of the
      Var(N-hat).</p>
    <p>In cell I18 type "Lower", and in J18 subtract 1.96*se from N-hat.</p>
    <p>In cell I19 type "Upper", and in J19 add 1.96*se to N-hat.</p>
    <p>You now have an estimate of population size and a 95% confidence interval
      for it.</p>
    <h2>Likelihood-based estimate of population size for two capture events</h2>
    <p>The Lincoln-Petersen method has a long history, but it isn't considered a
      modern method of estimating population size. It doesn't assume that all
      individuals are captured, or that all marked individuals are recaptured,
      but even though it's possible to express the estimator in terms of
      encounter probability (n<sub>2</sub>/p), it isn't easily used in a
      rigorous statistical framework. Modern, likelihood-based methods allow us
      to not only derive estimates of population size, they allow us to evaluate
      hypotheses both about the characteristics of our sampling procedure (i.e.
      trap happiness or trap shyness), and about the ecology of the organisms we
      are working with. The basic approach we will learn next can be modified to
      allow multiple trapping periods, to allow open populations, to estimate
      survival probability, and to accommodate differences in capture
      probabilities from time to time, or after first capture.</p>
    <p>We are going to start with just two periods because it's the simplest
      possible mark/recapture study design. The down side of starting this
      simple is that many of the important questions we could ask about our
      sampling (like, whether the animals are trap shy or trap happy) can't be
      addressed without at least three sampling intervals. We will end up with
      just enough degrees of freedom to estimate the two parameters we want,
      which are the probability that an animal will be captured (p) and
      population size. We will have to accept our population size estimate under
      the assumption that this single capture probability is always the same,
      between the first and second trapping period, and for animals that are
      caught for the first time as well as for animals that are recaptured. As
      ecologists, this should trouble you. But, we'll keep it simple today, and
      we will learn next week how to evaluate these important questions, when we
      add back the June capture period, and gain the needed degrees of freedom
      to test for differences in p under different circumstances.</p>
    <p>1. <strong>Set up a worksheet for ML estimation</strong>. In the blank
      worksheet called "ML Estimates", copy the capture histories and their
      frequencies from your PivotTable of frequencies of histories, and
      paste-special just the values to cell A1. Re-name column for the histories
      to "Histories", and for the frequencies to "Frequency". In cell A5 you can
      type '00, which is the history for animals never observed (notice the '
      before the 00 - this is needed to have Excel display 00 as a label). Leave
      its frequency blank.</p>
    <p>We are going to estimate two parameters from these data. The first is the
      probability of capture, which we will call "p". The next is the frequency
      of the "00" capture history, which we will call f(00) (that is, the number
      of animals never captured). Since there are only four possible capture
      histories, 10, 11, 01, and 00, and every animal in the population must
      have one of them, once we have an estimate for the frequency of f(00) we
      can estimate population size as the sum of the frequencies of all these
      histories.</p>
    <p>In cell D1 type "Parameter". In cell D2 type "p", and in cell D3 type
      "f(00)".</p>
    <p>In cell E1 type "MLE". This is where the estimates will go. But, we are
      not going to enter starting values here, because we are going to use
      something called a "link function" for each one, and we have to set that
      up first.</p>
    <p>In column F1 type "Betas". These will be the values we actually have the
      Solver vary, as these will be the parameters for our link functions. So,
      what's a link function, you ask?</p>
    <p>Solver works by iteratively changing your input cells, and observing how
      the change affects the objective cell. Our values for p and f(00) have
      some constraints on them - p is a probability, and can't exceed 0 or 1,
      and f(00) is a frequency, so it can't be negative. We can specify
      constraints for these in Solver to keep them in bounds, but Solver can
      still run into problems finding solutions if it ends up anywhere close to
      the boundaries. To help Solver do its work, we can pick functions that
      have parameters that are not bounded, but give results that are. So, for
      example, we can use the sin function, which takes any number on the number
      line and returns values that are between -1 and 1, add one to it to put
      the result between 0 and 2, and then divide the sum by 2 to get a result
      that's between 0 and 1. That way no matter what value Solver picks, it
      will end up with a value for p that is between 0 and 1.</p>
    <p>So, in cell F2 type a starting value of 0.5. Now, in cell E2 type
      =(sin(f2)+1)/2. This will give you a number that's between 0 and 1, which
      will be used as a starting value for the MLE for p.</p>
    <p>For the f(00) we need a different link function, because we need to allow
      the frequencies to go as high as needed, but never go below 0. The exp()
      function is a good choice - this is the "log link", because exp() raises
      the base of the natural logs (e) to the power of whatever is entered as an
      argument. In cell F3 type a 2 as an initial value for the beta, and in
      cell E3 type =exp(f3). Negative exponents, like e<sup>-β</sup>, are
      equivalent to 1/e<sup>β</sup>, so as the beta value becomes increasingly
      negative the function approaches 0 but never becomes negative.</p>
    <p>2. <strong>Calculate the probabilities of each history</strong>. We have
      a set of counts for our capture histories, which we are going to treat as
      having come from a multinomial distribution. The multinomial probability
      distribution predicts the probability of a distribution of frequencies for
      two or more discrete outcomes. The multinomial probabilities, then, are
      the probabilities of the capture history. Type "Multinomial probability of
      history" in cell H1.</p>
    <p>We are going to estimate these probabilities using just our estimate of
      p, the encounter probability. For the first history, 01, the probability
      of not being encountered in the first sample is (1-p), and the probability
      of being encountered in the second is p. Thus, the probability of this
      history is the product (1-p)*p. So, for the probability of the 01 history,
      in cell H2 type =(1-e2)*e2.</p>
    <p>You should now be able to fill in the rest of the probabilities yourself,
      including for 00. Where there is a 1 use e2 as the probability, and where
      there is a 0 use (1-e2) as the probability.</p>
    <p>3. <strong>Calculate the likelihood function</strong>. If you recall
      from lecture, the likelihood function has two parts: the multinomial
      coefficient, and the probability of each possible outcome. We just
      calculated the probability part, so let's do the multinomial coefficient
      part now. </p>
    <p>The multinomial coefficient is a very large number, that is calculated
      from even bigger numbers. It includes the factorial of the total number of
      animals captured, which means we can't calculate it directly because Excel
      is not capable of calculating factorials for numbers over 170 (try it - in
      an empty cell type =fact(170), and in another empty cell type =fact(171)).
    </p>
    <p>Fortunately for us, we don't actually need to calculate the likelihood,
      we only need the log-likelihood. The log of a big number is a much smaller
      number, and it turns out we can calculate the log of a factorial in Excel
      with no problem using the gammaln() function. We are also going to
      simplify the calculation by dropping all the terms that don't depend on
      f(00).</p>
    <p>The f(00) estimate shows up in two ways in the multinomial coefficient.
      The numerator of the coefficient is N!, but the total number of
      individuals includes the known total individuals trapped so far (which is
      the sum of the frequencies of the histories other than 00) as well as the
      unknown f(00). We can thus express N! as (M<sub>t</sub><sub>+1</sub> +
      f(00))!, with M<sub>t+1</sub> representing the total animals know to have
      been captured. The denominator has f(00) in it directly, since it is the
      product of the factorials of each of the frequencies. On a log scale this
      product of factorials become a sum of logs of factorials, and we can drop
      the logs of the factorials for the three known frequencies.</p>
    <p>So, with these changes we are left with needing to calculate the log of
      (M<sub>t</sub><sub>+1</sub> + f(00))!/f(00)! can be expressed as ln((M<sub>t</sub><sub>+1</sub>
      + f(00))!) - ln(f(00)!). All we need to do now is calculate this number.</p>
    <p>We can use the ML estimate for f(00), which will be in cell E3, but we
      need to calculate M<sub>t</sub><sub>+1</sub>. In cell A7 type "Mt+1", and
      in cell A8 type =sum(b2:b4).</p>
    <p>Next, in cell B7 type "Mult coeff.", and in B8 type
      =gammaln(a8+e3+1)-gammaln(e3+1). The gammaln() function gives the natural
      log of the "gamma function", which is another useful probability
      distribution. Adding one to the frequencies and then using this function
      on the total gives the log of the factorial of the frequencies.</p>
    <p>The next step is to calculate the probability part for the likelihood
      function, which will be based on the multinomial probabilities we
      calculated above. In cell C7 type "Prob. portion". The probability portion
      is equal to the probabilities of each outcome raised to the power of their
      frequencies, then multiplied together. Since we are calculating log
      likelihood, we can calculate this portion as the sum of the frequencies
      multiplied by the log of the probabilities. For the final probability of
      the un-observed 00 history, use the estimate of f(00) in cell E3 as the
      frequency. So, in cell C8 type =b2*ln(h2)+b3*ln(h3)+b4*ln(h4)+e3*ln(h5).</p>
    <p>Now, to get the log-likelihood, combine the multinomial coefficient and
      probability portions. In cell D7 type "LnLikelihood", and in cell D8 type
      =b8+c8.</p>
    <p>Congratulations! You're all ready to estimate p and f(00).</p>
    <p>4. <strong>Run Solver to get your estimates</strong>. To obtain the
      estimates, you will want to set the log likelihood in cell D8 to maximize
      by changing the betas in F2 and F3. We're maximizing because this is the
      log-likelihood, not the negative log-likelihood. </p>
    <p>You can set as a constraint on F2 that it should stay between 1.66 and
      -1.66, because the sin function will give all possible values between 0
      and 1 within this range. You'll need to set two constraints, one in which
      F2 is &gt;= -1.66, and one in which F2 is &lt;= 1.66.</p>
    <p>Run Solver and get your estimates.</p>
    <p>5. <strong>Interpret the results</strong>. You now have MLE's for p and
      f(00). The value for p tells you the chances that an animal would be
      trapped - the implication is that 27.3% of the animals in this population
      were trapped in each capture period.</p>
    <p>The population size estimate is actually the sum of M<sub>t+1</sub> and
      f(00), which we haven't calculated yet. You can write "N-hat" in cell d5,
      and in e5 add these numbers. You'll see the estimate is not identical to
      the LP estimate, but it's fairly close.</p>
    <p>You can look at the multinomial probabilities to see the probabilities of
      each capture history. For this population, 52.8% of the population was
      never captured, and thus 47.2% was captured at least once during the
      study.</p>
    <p>The other thing you can look at is the "goodness of fit" of the model to
      the data. The frequencies we would expect are the multinomial
      probabilities multiplied by N-hat. The multinomial probabilities of the
      histories are based completely on the value of p, and if you look at the
      probabilities, you'll see that we would expect the 01 history (with a
      probability of (1-p)p) and the 10 history (with a probability of p(1-p))
      to have the same frequencies based on these probabilities, but they don't
      - there were 28 more animals with a 01 than with a 10 history. </p>
    <p>To see how different the observed and expected values are from one
      another, we can calculate the expected numbers by multiplying the
      probabilities by N-hat. In cell I1 type "Expected", and in I2 type
      =e$5*h2. You'll see that we got pretty close to the observed frequency of
      11, but had to split the difference between 10 and 01. This is the best we
      can do with only two capture periods and a single value for p, but next
      week we will learn how to test for various ways that encounter
      probabilities can differ.</p>
    <p>6. <strong>Calculate 95% CI's, and look at the effects of sample size</strong>.
      We're going to look at how sample size affects our estimates by
      calculating a profile likelihood confidence interval for N-hat. We'll
      calculate the 95% confidence interval for our actual sample size, and then
      we'll double the frequencies of 01, 10, and 11 so that p will be
      unchanged, and then we'll calculate the 95% CI's again.</p>
    <p>To do this, we are going to vary the estimate for f(00), until the
      log-likelihood increases by 1.92 units. This will give us a confidence
      interval for f(00), but since N-hat is estimated as M<sub>t+1</sub> +
      f(00), we can get the confidence interval for N-hat by just adding M<sub>t+1</sub>
      to the upper and lower limit estimate. We are already adding M<sub>t+1</sub>
      to f(00) in cell E5 when we calculate N-hat, so we can record the value of
      N-hat to get our upper and lower limits for the estimated population
      size.&nbsp;</p>
    <p>In cell A13 write "Est.", and in cell B13 paste-special the current
      estimate of N-hat in cell E5.</p>
    <p>In cell A14 write "Lower", and in cell A15 write "Upper". </p>
    <p>In cell B12 write "Original n". In cell C12 write "Double n". We'll be
      filling in our lower and upper bounds in this block of cells, for our
      current sample size and for twice our current sample size.</p>
    <p>In cell A17 write "Width", and in cell A18 write "Width/est.". </p>
    <p>Next, copy the LnLikelihood and paste-special its value in cell D10. In
      cell C10 write "ML" - this is the actual maximum of the likelihood
      function. Next, in cell G10 type =-(d8-d10). This is the negative of the
      difference between the current value of the likelihood function given the
      parameter estimates (d8) and the maximum of the likelihood function (in
      d10). If you recall from lecture, this difference will equal 1.92 when the
      value of f(00) is either at its upper or lower limit for the 95% CI. In
      cell F10 type "Diff.".</p>
    <p>Now, to find the upper and lower limits, we need to find the two values
      of f(00) that make this difference equal to 1.92. We can use Solver to do
      this. We will keep p at its MLE value, and only vary f(00), and ask Solver
      to set cell G10 to 1.92. </p>
    <p>To get this started, displace the MLE for f(00) slightly above its
      current value - set the beta to 5.1.</p>
    <p>Next, start Solver, and tell it to set cell G10 to a value of 1.92 by
      changing ONLY cell f3 (do NOT change f2, because we want to hold p
      constant). Have Solver find your estimate.</p>
    <p>Since we displaced the beta for f(00) slightly above its MLE, you should
      end up with an estimate that's higher than before. This is your upper
      limit, so copy the new N-hat in cell E5 and paste-special the value to
      cell B15.</p>
    <p>Now, to get the lower limit you need to displace f(00) to be slightly
      below the MLE estimate - set the beta to 5, and run Solver again. Since we
      started below the MLE Solver should find the lower limit this time. Copy
      and paste-special the value for N-hat to the Lower limit (cell B14).</p>
    <p>Now, double the sample size. Change the frequencies from their original
      values (76, 48, and 23) to twice those amounts (152, 96, and 46). You'll
      see that all the cells updated for the likelihood function. You'll need to
      calculate the MLE's for p and f(00) again so you can get the ML for cell
      D10 - run Solver, and maximize D8 by changing F2 and F3 again. When you do
      this you'll see that p is exactly the same, and f(00) should be twice what
      it was before.</p>
    <p>Copy the N-hat for this larger sample size and paste-special its value to
      cell C13. Copy the LnLikelihood in cell D8 and paste-special its value to
      D10 - this is the new baseline for this new, larger sample size.</p>
    <p>You can now use Solver again to find the upper and lower limits. Use 5.9
      for the upper limit starting beta, and 5.7 as the lower limit starting
      beta. Don't forget to copy/paste special the value of N-hat each time.</p>
    <p>Finally, to see what increasing the sample size did, subtract the lower
      from the upper in the Width row - that is, in cell B17 type =b15-b14, then
      copy the formula to cell C17 as well. Then, in cell B18 divide the widths
      you just calculated by the estimates of N-hat - that is, in cell B18 type
      =b17/b13, and copy/paste it to cell C18.</p>
    <p>You'll see that the actual width of the interval for N-hat gets bigger
      with a larger sample size, because the total population size is twice as
      large. However, the width of the interval is a smaller fraction of the
      population when we use a larger sample size. If we had estimated the
      intervals for p we would have seen a straightforward decrease in interval
      width with increasing sample size, since p would not have changed value.
      In general, the precision of estimates will be better with a larger sample
      size.</p>
    <p>That's it for today. Save your Excel sheet and upload it to the course
      web site.</p>
  </body>
</html>
