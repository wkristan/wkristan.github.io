<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8">


    
    <title>Classic model selection</title>
    <link href="https://wkristan.github.io/style.css" rel="stylesheet" type="text/css">
    <script type="text/javascript" src="https://wkristan.github.io/main.js"></script>
  </head>
  <body>
    <div id="header">
      <div style="float: left"><button onmouseover="navToggle()">â˜°</button></div>
      <h1>Traditional model selection</h1>
    </div>
    <div id="navigation" style="display:none" onclick="navToggle()">
      <p><a href="#intro">Introduction</a></p>
      <p><a href="#exercise">Exercise</a></p>
      <p><a href="#calories">Modeling calories</a></p>
      <p><a href="#graham_help">Help with Graham paper</a></p>
      <p><a href="#model_sel">Model selection</a></p>
    </div>
    <div id="content">
      <p class="part" id="intro">Model selection is the process by which we
        choose the best statistical representation of the <strong>structure</strong>
        in our response data. Structure refers to the predictable patterning in data, which can be due to anything that is not purely
        random variation between individuals. A major cause of structure in
        response data (and the part of the structure that we are most interested
        in scientifically) are the <strong>fixed effects</strong> of predictor variables.</p>
      <p class="part">Model selection procedures are not always a necessary part of data
        analysis. For example, when all of the predictors in a model are
        experimentally interesting, are orthogonal by design, and have treatment
        levels that can be randomly assigned to subjects, then model selection
        becomes very simple - a single statistical model that matches the
        experimental design is used to test for statistical significance of the predictors, and if needed the non-significant terms can be dropped. Scientists who only ever analyze
        experimental data with these properties often think of their linear
        models just as tests of effects of each experimental treatment, and the
        fact that they are building a statistical model is something they don't really need
        to think about.</p>
      <p>However, when we are conducting exploratory analysis of data sets with
        many possible predictors which are not independent of one another (possibly strongly correlated), which may have complex inter-dependent
        relationships with the response, and with subjects that can't be
        randomly assigned the values of their predictors, then the fact that
        we're fitting a model to our data becomes important, because the model we use will determine how we interpret our data. With studies subject to these complications
        we definitely need to use the principles of model selection so that we
        base our understanding of our biological system on a good, informative
        model.</p>
      <p>Field ecologists that routinely work with observational data will
        nearly always need to approach their analysis as a model selection
        exercise. However, even lab-based experimental work can benefit from
        this perspective if the study involves nuisance variables (e.g. observer
        effects, growth chamber differences), covariates that may not be
        orthogonal with other predictor variables (e.g. mass of individuals,
        body temperature fluctuation), or factors that can't be randomly
        assigned to subjects (e.g. sex, strain).</p>
      <p>Today we will focus on using model selection to find the best set of
        predictors for the caloric content of plant-based foods, using adjusted
        R<sup>2</sup> as the criterion. Adjusted R<sup>2</sup> is better than
        multiple R<sup>2</sup> for model selection, because it balances model fit against model complexity.</p>
      <h2 class="part" id="exercise">Model selection with multiple predictors</h2>
      <p> I have extracted a selection of 142 raw, plant-based foods from the 67,000 foods found in
        USDA's massive National Nutrient Database (which you can access <a href="https://fdc.nal.usda.gov/">here</a> if you're interested). The records are all normalized to a 100 g sample, and includes information about the calories in the food (in kilocalories, or kcal), as well as comprehensive analysis of the food's nutrient composition.</p>
      <p> </p>
      <p> </p>
      <p> </p>
      <p> Start a new R Studio project for today. The data can be downloaded <a href="macronutrients.xlsx">here</a>, and the R markdown file is <a href="model_selection1.Rmd">here</a>.
      </p>
      <p>When you import the data make sure you use data.frame(read_excel()) to
        convert the tibble into a data frame, or some of the graphing steps
        below won't work correctly. Import the Excel file into a data set named
        <strong>food</strong>, and open it - you will see that the variables
        are:</p>
      <p> </p>
      <p> </p>
      <p> </p>
      <ul>
        <li> food - the name of the food</li>
        <li>kcal - kilocalories in the sample, to be used as the <strong>dependent
            variable</strong> for all models.</li>
        <li>ash - weight of inorganic residues (g) that are left behind when
          caloric content is measured in a bomb calorimeter.</li>
        <li>carb - grams of carbohydrate.</li>
        <li>fiber - grams of indigestible plant material, mostly cellulose, in
          the food. Fiber is not digestible by humans, and does not contain any
          caloric value.</li>
        <li>protein - grams of protein.</li>
        <li>fat - grams of fat</li>
        <li>water - grams of water. Water does not contain any caloric value. </li>
      </ul>
      <p>Each of the possible predictor variables also has a transformed
        version, called log.variable.name for each variable but water, which is
        called logit.water.</p>
      <p>Before any model selection begins, it's a good idea to make some graphs
        to start developing an understanding of the patterns and potential
        issues with the data set.</p>
      <ul>
      </ul>
      <p> </p>
      <p> </p>
      <p> </p>
      <p>1. First, we will <strong>make histograms of the variables</strong>.
        To save space we'll put them in an array, like we did for our residual
        plots in the model criticism exercise, but we don't want to plot all 14
        of the variables together or we won't be able to see them. We'll do a
        separate set of histograms for the un-transformed variables, and for the
        transformed variables.</p>
			<blockquote>
        <p>But...</p>
<p>We just finished learning that only the distribution of the residuals matters, which changes depending on the model - ergo, it doesn't make sense to test assumptions before fitting models. Or, so I told you just a week ago.</p>
<p>It is still true that the ultimate decision about whether we meet model assumptions or not will be based on the residuals, but variables that are strongly skewed will generally require a transformation - decisions about model structure (adding or omitting a variable, including or omitting an interaction, etc.) will usually not correct for a strongly skewed data distribution. Starting with an assessment of whether a linear scale or log scale gives us better data distributions is a good first step.</p>
<p>So, these graphs won't take the place of an assessment of model assumptions based on residuals, but they will help us start closer to the final model, and save us some time and trouble.</p>
      </blockquote>
      <p>In the chunk plot.untransformed.histograms of your <strong>Rmd file</strong>, set up the
        layout - we'll have 7 variables to plot in each set of histograms, and
        it's good to give them a little height so we can see the shape of the
        distribution better, so we'll use 2 rows and 4 columns to fit them all
        in:</p>
      <p class="rcmd">oldpar &lt;- par()</p>
      <p class="rcmd">par(oma = c(0,0,3,0), mfrow = c(2,4))</p>
      <p>We can take advantage of lapply() to get the graphs all created with a
        single command - first we need a list of the un-transformed variables we want to plot (put this in the next row
        of the same chunk):</p>
      <p class="rcmd">untrans.var &lt;-
        c("kcal","ash","carb","fiber","protein","fat","water")</p>
      <p>Then we can use lapply() on this list to make our 7 histograms with one
        command - these graphs don't need to be publication quality, they are
        just to guide our model building decisions, so the R base graphics
        command hist() will do the job:</p>
      <p class="rcmd">lapply(untrans.var, FUN = function(x) hist(food[ , x],
        main = x, xlab = x)) </p>
      <p>You should see these histograms:</p>
      <img src="untrans_histo.png" alt="Untrans">
      <p>The lapply() command takes the list of variables, untrans.var, and
        works through the variable names within it one at a time. The FUN = function(x)
        statement defines the plotting command to use, which is hist(data.set[
        , name.of.variable], main = title.of.plot, xlab = label.on.x.axis). The
        value x comes from the untrans.var list, so it is just the name of a
        variable. If we type the command in the <strong>Console</strong>:</p>
      <p class="rcmd">hist(food[,"kcal"])</p>
      <p>you will see the kcal histogram in the plot window. We can add the
        title and x-axis label with (still in the <strong>Console</strong>):</p>
      <p class="rcmd">hist(food[,"kcal"], main = "kcal", xlab = "kcal")</p>
      <p>This gives us the same histogram as the first one in our
        lapply()-generated set - the only difference when we do this with our FUN in lapply() is that we use x as the variable
        name, which comes from the elements of untrans.vars - each time lapply() moves on to a new variable name it is assigned to x, and we get a new graph with that name using the same hist() command each time.</p>
      <p>You may notice that the header block in the code chunk reads {r
        plot.transformed.histograms, results='hide'} - the results='hide' part
        tells R to not include all the numerical output from hist(), and just
        plot the graphs themselves. The graphs still get produced without this setting, but
        the knitted output includes pages of output neither of us needs to see - hiding this unnecessary output makes the knitted file better.</p>
      <p>So, back to the histograms - nothing looks remotely normally
        distributed, so we can look at the transformed variables next. Go ahead
        and try to do this in chunk plot.transformed.histograms of your <strong>Rmd
          file</strong>. If all goes well you should see this:</p>
      <img src="transformed_vars.png" alt="Transformed histograms">
      <p>The transformed versions are obviously much closer to normally
        distributed, so we might expect that transformations will be needed. We have a second mode in log.kcal, but we don't need to worry about that yet - there is a second mode in several of the nutrient histograms as well, so it's likely we'll be able to predict those high calorie foods with combinations of high carb, protein, and fat combined with low water, and the residuals around those predictions could very easily be normally distributed.</p>
<p>The
        <strong>logit transformation</strong> was applied to water data to
        produce water.logit. The logit of a proportion or a probability is the
        log odds ratio, or ln(p/(1-p)), where p is the proportion of the sample
        that is water (since the sample is 100 g, p = water/100). The logit
        transformation works well for proportions (or percentages converted to proportions), provided that there are no 0% or 100% values.</p>
      
      <p>Now that we know that transformations were needed to improve the distribution of the data we can see how the variables are inter-related.</p>
      <p> </p>
      <p> </p>
      <p> </p>
      <p> </p>
      <p> </p>
      <p> </p>
      <p> 2. <strong>Make a scatterplot matrix of all of the variables</strong>. Looking at the relationships between variables is also a good early step in model building. GLM assumes linear relationships between predictors and response, so we can see how each predictor individually relates to caloric content of the food. Correlations between predictors are a complication we need to be aware of as well.</p>
      <p>We will use a scatterplot matrix for this - a scatterplot matrix is structured a lot like a correlation matrix, with the variables placed both in the rows and columns. In every cell of the matrix a row and column variable meet, and the row variable is used as the y-axis while the column variable is used for the x-axis of a scatterplot.</p>
<p>Since the upper triangle and the lower triangle of the scatterplot
        matrix are mirror images of each other, we can make better use of the
        lower triangle by putting the correlation coefficients and p-values in
        them. R allows us to define functions and then use them within the
        session. <strong>Run the panel.cor.function</strong> chunk to get the function added to
        your workspace (you should see it in the Environment tab if all goes
        well). Note that a version of this function can be found in R libraries - the xcms library has one, for example, which you could use instead of adding the panel.cor() function to your environment every time you need it.</p>
      <p>Since we will either use kcal or log.kcal as the response variable we
        can make two different sets of scatter plots that include just one or
        the other version of kcal. To make the version with the un-transformed
        version of kcal use (in the kcal.untrans.pairs.plots chunk of your <strong>Rmd
          file</strong>):</p>
      <p class="rcmd">pairs(food[,untrans.var], lower.panel = panel.cor)</p>
      <p>Having made a list of untransformed variables when we made histograms,
        we can use it again here to identify the variables we want to include in
        our scatterplot matrix.</p>
      <img src="splom_untrans.png" alt="Untransformed">
      <p>If you have a hard time seeing your own pairs plot you can use the
        "Display in own window" button in the upper right corner of the output
        block, <img src="own_window.png" style="display: inline">, to get the
        scatterplots to pop out into a larger, resizable window.</p>
      <p>The pairs() command makes a scatterplot matrix with names of the
        variables in the diagonal. Variables are used as the y-axis for the row
        they appear in, and the x-axis for the column they appear in. For this
        example, kcal is the y-axis for the first row of scatterplots, and you
        can use that first row of graphs to see how kcal is related to all of
        the the predictor variables, one at a time. The relationship between the
        predictors (and thus how much of a problem with confounding to expect
        between them) is illustrated by the rest of the scatterplots. The lower
        triangle has the correlation
        coefficient and p-value for each pair of variables. The
        correlations of each predictor with kcal is in the first column, and the
        correlations between the predictors is in all the other positions.</p>
      <blockquote>
        <h3> Convenience vs. clarity </h3>
        <p> Frequently, there are multiple ways of accomplishing the same thing
          in R, and deciding which approach to take is arbitrary to some extent -
          if the end result is the same, does it really matter how you get it
          done?</p>
        <p>For example, we could write our pairs() command like this:</p>
        <p class="rcmd">pairs(food[ , 2:8], lower.panel = panel.cor)</p>
        <p>This is convenient because the numbers 2:8 refer to columns 2 through
          8 of the food data set - using this method gets the plot made without
          having to make a list of variables at all. But, it has the
          disadvantage of being opaque to somebody reading your code who does
          not have the data set in hand, because the names of these column numbers
          appear nowhere in your Rmd file. And it's not just some hypothetical third party that could be confused by this - you may not yet have had the
          embarrassing experience of opening up an Rmd file, knowing that you
          wrote all the code in it, and not being able to remember why you did
          what you did, but that day is coming! Writing code that is hard
          to understand makes it more likely this will happen to you.</p>
        <p>By far the clearest, most "self-documenting" way to write the command
          would be:</p>
        <p class="rcmd">pairs(food[ ,
          c("kcal","ash","carb","fiber","protein","fat","water")], lower.panel =
          panel.cor)</p>
        <p>This style makes it perfectly clear to anyone
          who reads this line of code which variables you're plotting. But, the
          disadvantage is that it is less convenient - it involves more typing, and is an opportunity for
          typos (and the elevated blood pressure that goes with them). Also, if
          you use this approach and realize you should have included another
          variable in the list, you would need to update the list in every line
          of code in which you used this list of variables.<span style="font-family: &quot;Open Sans&quot;,sans-serif;"><br>
          </span></p>
        <p><span style="font-family: &quot;Open Sans&quot;,sans-serif;">None of
            these approaches is wrong, and they all work. </span>Personally, I
          have gravitated toward the approach I had you use above, in which we first
          make a list of variables in one code chunk and then use it as an argument wherever we need it in later commands. It's really
          useful to be able to change one element in the list in one line of
          code, and then re-run all the code that uses the list as an argument, rather than having to make the change in every line of code that uses the list of variables - for example, if we decided
          to drop protein from the list, we could just re-write the line
          in which we made the untrans.var object, and then re-run the
          histograms and pairs plot to update them with the new list that omits protein. The list of variable used appear in the code chunk where the list was created, which makes the code more self-documenting than if we used ranges of column numbers.</p>
        <p>When a list of variables is only used once, like will be the case in
          our next two pairs() plots, I just write the list of variable names so
          that the code is easier to understand. I used to prefer using column
          numbers because of the convenience, but over time have found the lack
          of clarity to be a liability, and I don't use that method very often
          anymore. </p>
      </blockquote>
      <p>We can graph how kcal relates to the log-transformed versions of the
        predictors with (in chunk kcal.trans.pairs.plots of your <strong>Rmd
          file</strong>):</p>
      <p class="rcmd">pairs(food[,c("kcal","log.ash","log.carb","log.fiber","log.protein","log.fat","logit.water")],
        lower.panel = panel.cor)<br>
      </p>
      <p>You'll see that several of the relationships look more linear using
        transformed predictors.</p>
      <p>We can also choose to only log-transform the response variable (in the
        log.kcal.untrans.pairs.plots chunk of your <strong>Rmd file</strong>):</p>
      <p class="rcmd">pairs(food[,c("log.kcal","ash","carb","fiber","protein","fat","water")],
        lower.panel = panel.cor)</p>
      <p>Now, make the pairs plot that compares log.kcal to the transformed
        predictors (in the log.kcal.trans.pairs.plots chunk of your <strong>Rmd
          file</strong>).</p>
      <p>2. <strong>Pick the scale to use for the variables.</strong> Now that
        we have all of the scatterplot matrices, let's put them to use. Our
        first concern is to make sure our predictors are linearly related to the
        response, so look at each of the scatterplot matrices first to see which
        of the first rows of scatterplots gives us relationships that look the
        most linear - these are the graphs with calories on the y-axis.</p>
      <p>Secondly, we should pick versions of the variables we think will let us
        meet the distributional assumptions of GLM. There is a lot we can't tell
        from pairs of variables plotted against one another, but to the extent
        that the plots give us an even distribution of data along the x-axis,
        and what looks like an even distribution of data along the y-axis, we
        will have a better chance of meeting GLM assumptions when we use the
        variables in a model.</p>
      <p>Based on these initial considerations, you'll see that the most linear
        relationships with the best data distributions come from the graphs with transformed response (log.kcal) and transformed predictors. Between our scatterplots and our histograms we're better off using transformed variables.</p>
      <blockquote style="background: #f5f5fb; border: 1px solid; padding: 10px;">
        <p>We are going to be working with models that have a log-transformed
          response as well as log-transformed predictors today. This gives us a
          good excuse for learning about how to interpret models with log
          transformed predictors, responses, or both.</p>
        <p>When you log-transform both the response and predictor variables for
          a regression analysis as we will be doing today, the model you are
          fitting is:</p>
        <p>log y = b log x + log a</p>
        <p>Back-transforming both sides of the equation to the linear scale that
          the data were measured on gives us a <strong>power function</strong>:</p>
        <p>y = ax<sup>b</sup></p>
        <p>The value of y being predicted is the geometric mean of kcal.</p>
        <p>A model with the response on a log scale, but with the predictor on a
          linear scale, like this:</p>
        <p>log y = b x + k</p>
        <p>became an <strong>exponential</strong> function when it's
          back-transformed:</p>
        <p>y = a(e<sup>bx</sup>)</p>
        <p>Notice that these functions differ in where the x-variable is placed
          - in an exponential function the x variable is in the exponent for the
          base of the logarithm, whereas power functions use x as a base and
          raise it to an exponent. Since we fit the model with log y as the
          response, this equation is also predicting the geometric mean on the
          original, linear data scale.</p>
        <p>The switch from using the letter k to the letter a is to avoid having
          a log-term on the right side of the equation. The letter k is used as
          the intercept of a straight line equation, and the intercept is a
          constant that is estimated from the data. After back-transforming a is
          also a constant estimated from the data, but it is a constant
          multiplier instead of being additive. The intercept, k, is equal to
          log a, so it is equally correct to write the straight line equation as
          log y = b x + log a, but since log a is just a constant, and we're
          saying that the right side of the equation is on a linear scale, it's
          confusing to have log a there.</p>
        <p>Finally, we can log-transform the x variable but keep the y-variable
          on a linear scale. The model we used was:</p>
        <p>y = b log x + k</p>
        <p>This model is predicting the a arithmetic mean of y, it's just using
          the log of x to do it. If you back-transform this model it becomes a <strong>logarithmic</strong>
          function:</p>
        <p>e<sup>y</sup> = ax<sup>b</sup></p>
        <p>With this introduction as background, you can use graphs that have
          log-scale axes to get an idea of the form of the functional
          relationship between predictor and response. For example, if you make
          a scatterplot of kcal (y axis) against protein (x axis), using linear
          axes for both, using a log x-axis with a linear y-axis, using a log
          y-axis with a linear x-axis, or using log axes for both x and y, you
          get graphs like these:</p>
        <table width="100%" border="1">
          <tbody>
            <tr align="center">
              <td colspan="4" rowspan="1">
                <p>If your data look linear when you use these axis scales...</p>
              </td>
            </tr>
            <tr>
              <td style="text-align: center;">
                <p>Linear x and linear y</p>
              </td>
              <td style="text-align: center;">
                <p>Log of x, linear y</p>
              </td>
              <td style="text-align: center;">
                <p>Log of y, linear x</p>
              </td>
              <td style="text-align: center;">
                <p>Log of x and log of y</p>
              </td>
            </tr>
            <tr>
              <td style="text-align: center;"><img alt="Linear x and linear y" src="linear2.png"><br>
              </td>
              <td style="text-align: center;"><img alt="Log of x" src="log_x2.png"><br>
              </td>
              <td style="text-align: center;"><img alt="Log y" src="log_y2.png"><br>
              </td>
              <td style="text-align: center;"><img alt="Log x and log y" src="log_xy2.png"><br>
              </td>
            </tr>
            <tr align="center">
              <td colspan="4" rowspan="1">
                <p>...the relationship between y and x is:</p>
              </td>
            </tr>
            <tr>
              <td style="text-align: center;">
                <p style="height: 20px;">Linear relationship</p>
              </td>
              <td style="text-align: center;">
                <p>Logarithmic relationship</p>
              </td>
              <td style="text-align: center;">
                <p>Exponential relationship</p>
              </td>
              <td style="text-align: center;">
                <p>Power function relationship</p>
              </td>
            </tr>
            <tr>
              <td style="text-align:center">
                <p>y =bx + a</p>
              </td>
              <td style="text-align:center">
                <p>e<sup>y</sup> = ax<sup>b</sup></p>
              </td>
              <td style="text-align:center">
                <p>y = a(e<sup>bx</sup>)</p>
              </td>
              <td style="text-align:center">
                <p>y = ax<sup>b</sup></p>
              </td>
            </tr>
          </tbody>
        </table>
        <p>Based on these graphs, you get the most linear relationship on the
          graph from log-transforming both protein and kcal, so the relationship
          between protein and energy in food is a power function.<br>
        </p>
      </blockquote>
      
      <p>We're ready now to start building our models. We will start by fitting each of the single predictor models to give us
        a baseline for comparison, and then we will fit some multiple
        regressions with combinations of predictors to see if the added
        complexity improves the adjusted R<sup>2</sup>.</p>
      <p> </p>
      <p> </p>
      <p> </p>
      <p> 3. <strong>Fit all the one-variable models</strong>. We'll be fitting a bunch of models, so we will put them into an R <strong>list</strong> to help us keep track of them. Once we have all the models we want in the list, we can pull
        out the adjusted R<sup>2</sup> to compare them.</p>
      <p>First, let's make an empty list we can add models to (in the
        make.model.list chunk of your <strong>Rmd file</strong>):</p>
      <p class="rcmd">models.list &lt;- list()</p>
      <p>The list() function with no arguments makes an empty list that we can add our models to as we make them. If you find models.list
        in the Environment tab you'll see it's shown as a "List of 0", with no
        triangle next to it to reveal summary information - there's nothing to
        summarize yet.</p>
      <p>To add the first model to models.list use the command (chunk
        add.single.predictor.models.to.list in your <strong>Rmd file</strong>):</p>
      <p class="rcmd">models.list$log.ash &lt;- lm(log.kcal ~ log.ash, data =
        food)</p>
      <p>This command the now very familiar lm() command to fit the model, but the fitted model is assigned as a named element of models.list - assigning the output to models.list$log.ash makes an element called log.ash in models list, and assigns the fitted model to it. </p>
<p>If you look at the models.list in
        the Environment tab it is now a list of 1, and if you
        open it using the triangle next to the models.list object you will see a
        summary of the fitted model stored in log.ash. You'll see that log.ash
        is shown as a "List of 12", because fitted models are themselves R lists
        - we can store any object in an R list, including another list.</p>
      <blockquote>
        <p>A little more on using information stored in lists... we would like to be able to use our usual
          commands on models we assign to a list, such as summary() and anova(). This is simple to do, we just need to
          use name.of.list$name.of.stored.object to refer to the fitted model
          objects, instead of just the name of the fitted model alone.</p>
        <p>To see what is in the models.list object, you can type in the <strong>console</strong>:</p>
        <p class="rcmd">models.list</p>
        <p>which shows you the contents of the list:</p>
        <p class="rout">$log.ash<br>
          <br>
          Call:<br>
          lm(formula = log.kcal ~ log.ash, data = food)<br>
          <br>
          Coefficients:<br>
          (Intercept)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; log.ash&nbsp; <br>
          &nbsp;&nbsp;&nbsp;&nbsp; 4.9773&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          0.5827&nbsp; </p>
        <p>This shows the name of the element (with a dollar sign before it,
          $log.ash), and then the print() output for the object (that is, the output we would get if we used the command print(models.list$log.ash) - this is the same output we get when we type the name of an object in the console). We can get only the print() output without the name of the object using
          (in the <strong>console</strong>):</p>
        <p class="rcmd">models.list$log.ash</p>
        <p>which will display this output:</p>
        <p class="rout">Call:<br>
          lm(formula = log.kcal ~ log.ash, data = food)<br>
          <br>
          Coefficients:<br>
          (Intercept)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; log.ash&nbsp; <br>
          &nbsp;&nbsp;&nbsp;&nbsp; 4.9773&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          0.5827 </p>
        <p>This is just the output for the lot.ash object, and since we
          specifically asked for this object with our command we do not get the
          object name reported this time. </p>
        <p>If we ask for a summary() of the models.list object we get a summary
          of the properties of the list (in the <strong>console</strong>):</p>
        <p class="rcmd">summary(models.list)</p>
        <p>gives us:</p>
        <p class="rout">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Length Class
          Mode<br>
          log.ash 12&nbsp;&nbsp;&nbsp;&nbsp; lm&nbsp;&nbsp;&nbsp; list</p>
        <p>To get a summary of the log.ash fitted model object we would name it in
          the summary() command (in the <strong>console</strong>):</p>
        <p class="rcmd">summary(models.list$log.ash)</p>
        <p>which shows us the summary output for this fitted model:</p>
        <p class="rout">Call:<br>
          lm(formula = log.kcal ~ log.ash, data = food)<br>
          <br>
          Residuals:<br>
          &nbsp;&nbsp;&nbsp;&nbsp; Min&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          1Q&nbsp;&nbsp; Median&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          3Q&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Max <br>
          -1.73359 -0.63291&nbsp; 0.04934&nbsp; 0.55279&nbsp; 1.86214 <br>
          <br>
          Coefficients:<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          Estimate Std. Error t value Pr(&gt;|t|)&nbsp;&nbsp;&nbsp; <br>
          (Intercept)&nbsp; 4.97726&nbsp;&nbsp;&nbsp; 0.06852&nbsp;&nbsp;
          72.64&nbsp; &lt; 2e-16 ***<br>
          log.ash&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.58268&nbsp;&nbsp;&nbsp;
          0.09398&nbsp;&nbsp;&nbsp; 6.20 5.96e-09 ***<br>
          ---<br>
          Signif. codes:&nbsp; 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1<br>
          <br>
          Residual standard error: 0.8012 on 140 degrees of freedom<br>
          Multiple R-squared:&nbsp; 0.2154,&nbsp;&nbsp;&nbsp; Adjusted
          R-squared:&nbsp; 0.2098 <br>
          F-statistic: 38.44 on 1 and 140 DF,&nbsp; p-value: 5.959e-09</p>
        <span style="font-family: &quot;Open Sans&quot;,sans-serif;">We can add
          as many models as we want to this list, and access each one in the
          same way.</span><br>
      </blockquote>
      <p> </p>
      <p>Okay, returning to our regularly scheduled program... add the next
        model of log.kcal ~ log.carb to the list use the command: </p>
      <p class="rcmd">models.list$log.carb &lt;- lm(log.kcal ~ log.carb, data =
        food)</p>
      <p>If you type models.list you'll see that you now have two named
        elements, one called $log.ash and one called $log.carb, each containing
        the fitted model you assigned.</p>
      <p>Keep going - add single predictor models to the models.list for log.fiber, log.protein,
        log.fat, and logit.water. Watch the labeling - make sure you assign each
        model to a different predictor name in the models.list or you'll
        over-write one model with another. When you're done you should have six
        models in the list (check in your Environment tab that the models.list
        object is a List of 6), one for each of the six predictor variables.</p>
      <p>If you assign the wrong model to the list, but the label you used is
        okay, just assign the correct model to the same name again and the right
        model will over-write the wrong one. If you assign a model to the list
        with the wrong label, you can delete it using the command:</p>
      <p class="rcmd">models.list$the.wrong.label.you.used &lt;- null</p>
      <p> </p>
      <p> </p>
      <p> </p>
      <p>and then you can add the model back in using the correct label.</p>
      <p>4. <strong>Extract the r.squared and adj.r.squared from every model</strong>.
        Once you have a list with a set of six single-predictor models in it we
        can use lapply() again to extract the adjusted R<sup>2</sup> from each model. The command is a little complicated, so
        we'll build up to it one step at a time with commands in the <strong>Console</strong>
        until we are getting what we want, and then we'll put the final, correct
        command in your Rmd file.</p>
      <p>For example, we can use lapply() to get summary() output from each
        model (in the <strong>Console</strong>):</p>
      <p class="rcmd">lapply(models.list, summary)</p>
      <p>This command gives us summary output for each model in models.list.
        Used this way lapply() is simple - it takes a list as its first
        argument, and the name of a built-in function to apply to each named
        element as the second. This use of lapply() is equivalent to using
        summary(models.list$model.name) once for each model in the list, but
        with lapply() you just need to issue the command once to get all of
        them.</p>
      <p>Big improvement already - we now have summary output for every model,
        which includes the R-squared, adjusted R-squared, F, and p-values each
        model, which is all the information that we need. However, it's not
        reported in a very convenient format - we would still need to go through and find all of these values, and compile them into an output table by hand to compare them. Doing tasks like this by hand is tedious
        and error-prone, and resembles work, which is what computers are
        supposed to do for us! To save time and avoid typos it is better to
        figure out how to construct a function that will give us what we need in
        a nice, compact, organized form.</p>
      <p>First, we need a little information about the summary() output to know
        how best to extract the statistics we need. If we're lucky these
        quantities are named elements that we can pull out of the fitted model
        output. We can get a report of the named elements in an object by using
        the names() command - to see how the summary() output is
        structured, we'll use names() on the summary() of the first model in
        models.list, like so (still in the Console):</p>
      <p class="rcmd">names(summary(models.list$log.ash))</p>
      <p>You will see that the named elements in the summary() output are:</p>
      <p class="rout">&nbsp;[1]
        "call"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        "terms"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        "residuals"&nbsp;&nbsp;&nbsp;&nbsp; "coefficients"&nbsp;
        "aliased"&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;<br>
        &nbsp;[6] "sigma"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        "df"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        "r.squared"&nbsp;&nbsp;&nbsp;&nbsp; "adj.r.squared" "fstatistic"&nbsp;
        &nbsp;<br>
        [11] "cov.unscaled"</p>
      <p>There are named elements called "r.squared" and another called "adj.r.squared". We can confirm that these elements contain the
        values we need with the command (in the Console):</p>
      <p class="rcmd">summary(models.list$log.ash)$r.squared</p>
      <p>which should give you:</p>
      <p class="rout">[1] 0.2154293</p>
      <p>which is indeed the R<sup>2</sup> for the log.ash model. </p>
      <p>To use this summary() command in our lapply() function we can define
        our own function to be applied to each named element, like so (in the
        Console):</p>
      <p class="rcmd">lapply(models.list, FUN = function(x)
        summary(x)$r.squared)</p>
      <p>This will give you output like this:</p>
      <p class="rout">$log.ash<br>
        [1] 0.2154293<br>
        <br>
        $log.carb<br>
        [1] 0.8356839<br>
        <br>
        $log.fiber<br>
        [1] 0.4676565<br>
        <br>
        $log.protein<br>
        [1] 0.3557148<br>
        <br>
        $log.fat<br>
        [1] 0.3620739<br>
        <br>
        $logit.water<br>
        [1] 0.9109813</p>
      <p>This is fine, but this output is a list - we can get this output into a vector of numbers,
        labeled by the name of the model, with a different version of the
        apply() command, called sapply() - in the <strong>Console</strong>:</p>
      <p class="rcmd">sapply(models.list, FUN = function(x)
        summary(x)$r.squared)</p>
      <p>which gives us:</p>
      <p class="rout">&nbsp;&nbsp;&nbsp; log.ash&nbsp;&nbsp;&nbsp;
        log.carb&nbsp;&nbsp; log.fiber log.protein&nbsp;&nbsp;&nbsp;&nbsp;
        log.fat logit.water <br>
        &nbsp; 0.2154293&nbsp;&nbsp; 0.8356839&nbsp;&nbsp; 0.4676565&nbsp;&nbsp;
        0.3557148&nbsp;&nbsp; 0.3620739&nbsp;&nbsp; 0.9109813</p>
      <p>This is the same output we got with lapply(), but it's in a vector, which we can easily use to make a table of our results.</p>
      <p>This command is getting us our r.squared values from each model successfully, so
        let's put this into a command in our Rmd file (chunk r.squared of your <strong>Rmd
          file</strong>):</p>
      <p class="rcmd">sapply(models.list, FUN = function(x)
        summary(x)$r.squared) -&gt; r.squared</p>
      <p>We can use this same approach to get the adj.r.squared (in the
        adj.r.squared chunk of your <strong>Rmd file</strong>):</p>
      <p class="rcmd">sapply(models.list, FUN = function(x)
        summary(x)$adj.r.squared) -&gt; adj.r.squared</p>
      <p>We can now combine the r.squared and adj.r.squared vectors into a data
        frame to make them easier to interpret (in the make.adj.r.squared.table
        chunk of your <strong>Rmd file</strong>):</p>
      <p class="rcmd">data.frame(r.squared, adj.r.squared)</p>
      <p>which should give you:</p>
      <p class="rout">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        r.squared adj.r.squared<br>
        log.ash&nbsp;&nbsp;&nbsp;&nbsp; 0.2154293&nbsp;&nbsp;&nbsp;&nbsp;
        0.2098252<br>
        log.carb&nbsp;&nbsp;&nbsp; 0.8356839&nbsp;&nbsp;&nbsp;&nbsp; 0.8345102<br>
        log.fiber&nbsp;&nbsp; 0.4676565&nbsp;&nbsp;&nbsp;&nbsp; 0.4638540<br>
        log.protein 0.3557148&nbsp;&nbsp;&nbsp;&nbsp; 0.3511128<br>
        log.fat&nbsp;&nbsp;&nbsp;&nbsp; 0.3620739&nbsp;&nbsp;&nbsp;&nbsp;
        0.3575173<br>
        logit.water 0.9109813&nbsp;&nbsp;&nbsp;&nbsp; 0.9103455<br>
      </p>
      <p>You'll see that the adjusted R<sup>2</sup> are all very similar to the
        non-adjusted versions, because these are all single variable models. Of
        these, logit.water has by far the highest adjusted R<sup>2</sup> - if we
        were restricted to picking the best single predictor model, it would be
        the one with logit.water.</p>
      <p>But, we are not restricted to using a single predictor, and it's
        possible we can improve. We will fit some more complex models next for
        comparison.<br>
      </p>
      <p>5. <strong>Fit a model with log.ash and log.protein</strong>. The
        variables log.ash and log.protein are highly correlated. If the
        correlated part of these predictors is the part that explains variation
        in log.kcal, then including them both at the same time would not improve
        the adjusted R<sup>2</sup> compared to including just one or the other
        of the variables. </p>
      <p>Add a model (in chunk log.ash.log.protein.model) that uses log.ash +
        log.protein as predictors to your models.list, and name it <strong>log.ash.log.protein</strong>
        (that is, you'll assign it as models.list$log.ash.log.protein).</p>
      <p>In the same code chunk, run your sapply() commands to get r.squared and
        adj.r.squared extracted (just repeat the commands, the old versions of
        r.squared and adj.r.squared will be replaced with the new ones that
        include your new log.ash.log.protein model).</p>
      <p>Then, in the same chunk, make your data frame of r.squared and
        adj.r.squared again - you should see:</p>
      <p class="rout">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        r.squared adj.r.squared<br>
        log.ash&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        0.2154293&nbsp;&nbsp;&nbsp;&nbsp; 0.2098252<br>
        log.carb&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        0.8356839&nbsp;&nbsp;&nbsp;&nbsp; 0.8345102<br>
        log.fiber&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        0.4676565&nbsp;&nbsp;&nbsp;&nbsp; 0.4638540<br>
        log.protein&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        0.3557148&nbsp;&nbsp;&nbsp;&nbsp; 0.3511128<br>
        log.fat&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        0.3620739&nbsp;&nbsp;&nbsp;&nbsp; 0.3575173<br>
        logit.water&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        0.9109813&nbsp;&nbsp;&nbsp;&nbsp; 0.9103455<br>
        log.ash.log.protein 0.3557625&nbsp;&nbsp;&nbsp;&nbsp; 0.3464929</p>
      <p>Any time you add variables you'll get an increase in the unadjusted R<sup>2</sup>
        - you can see the unadjusted R<sup>2</sup> is a little higher than
        either the model with log.ash alone or log.protein alone. But, if you
        compare the adjusted R<sup>2</sup> for this model to the one you got for
        log.protein alone, and you'll see that it's a little lower than
        log.protein's adjusted R<sup>2</sup>. This happened because adding
        ash to the model doesn't explain much additional variation, but
        it increases the complexity of the model - thus, the penalty for complexity is greater than the benefit of additional explained variation for this model.</p>
      <p> 6. <strong>Fit a model with all of the predictors included</strong>.
        Use all six of the predictors in the next model, and assign it to
        models.list as an object called all.var.<span style="color: blue;"><span style="font-family: &quot;Ubuntu Mono&quot;;"> </span></span>Get an
        updated set of r.squared and adj.r.squared for that includes this new
        model (all in chunk all.variables of the <strong>Rmd file</strong>). </p>
      <p> </p>
      <p> </p>
      <p> This six variable model explains a lot of variation in comparison with
        the single-variable models according to the multiple R<sup>2</sup>
        values, and it has the highest adjusted R<sup>2</sup> of any of the
        models fit so far. However, this model is more complex than a
        single-variable regression, and we may be able to get a better
        adj.r.squared if we drop non-significant terms.</p>
      <p>Produce a Type II ANOVA table for the all.var model (in chunk
        type.II.ss.anova of the <strong>Rmd file</strong>). You will see that
        some of the predictors are not significant and/or have F-values less
        than 1. Predictors with F-values less than 1 generally decrease adjusted
        R<sup>2</sup>, and variables with F-values greater than 1 increase it
        (even if they are not statistically significant). Variables with
        F-values less than 1 are not explaining enough variation in log.kcal to
        justify including them in the model, so we should see how dropping these
        non-significant predictors affects adjusted R<sup>2</sup>.</p>
      <p> 7. <strong>Fit a model that omits non-significant predictors.</strong>
        Fit a model that drops the non-significant predictors from the
        six-variable model you just made, and assign it to models.list with the
        name "signif.pred" (in chunk signif.predictors.model of the <strong>Rmd
          file</strong>). You would expect the adjusted R<sup>2</sup> should
        increase very slightly when you drop non-significant predictors, even
        though you will explain somewhat less variation. Get a final updated set
        of model stats, which will look like this:</p>
      <p class="rout">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        r.squared adj.r.squared<br>
        log.ash&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        0.2154293&nbsp;&nbsp;&nbsp;&nbsp; 0.2098252<br>
        log.carb&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        0.8356839&nbsp;&nbsp;&nbsp;&nbsp; 0.8345102<br>
        log.fiber&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        0.4676565&nbsp;&nbsp;&nbsp;&nbsp; 0.4638540<br>
        log.protein&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        0.3557148&nbsp;&nbsp;&nbsp;&nbsp; 0.3511128<br>
        log.fat&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        0.3620739&nbsp;&nbsp;&nbsp;&nbsp; 0.3575173<br>
        logit.water&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        0.9109813&nbsp;&nbsp;&nbsp;&nbsp; 0.9103455<br>
        log.ash.log.protein 0.3557625&nbsp;&nbsp;&nbsp;&nbsp; 0.3464929<br>
        all.var&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        0.9734324&nbsp;&nbsp;&nbsp;&nbsp; 0.9722516<br>
        signif.pred&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        0.9732879&nbsp;&nbsp;&nbsp;&nbsp; 0.9727072</p>
      <p>You'll see that the best model is the one that only includes only
        significant predictors, since its adjusted R<sup>2</sup> is (slightly)
        higher than for any of the other models. Although the difference in
        adj.r.squared is tiny, the fact that the model with only statistically
        significant predictors out-performs a more complex model makes it a
        better choice for interpretation.</p>
      <p>8. <strong>Consider why the non-significant variables are
          non-significant</strong>. Look for a minute at the un-adjusted R<sup>2</sup>
        for the predictors that were not significant in the all.var
        model (log.ash, log.fiber, and log.protein). All of these have
        reasonably large R<sup>2</sup> values, and they are all statistically
        significant by themselves. This is a sign that they are correlated with
        other predictors, and are not significant when they are included in the
        model with other predictors. Since we dropped them to
        make the signif.pred model it's possible that they collectively explain
        a significant amount of variation, but because they are correlated with
        one another we couldn't see this when they are all included together.</p>
      <p>We can check this by testing the three variables that were dropped as a
        group. The simplest way of doing this in R is by comparing the model
        that included them (the full model) to the model that omitted them (the
        reduced model). In R this is simply a matter of using the anova()
        command with the reduced model as the first argument, and the full model
        as the second. So, for example, you can use (in chunk
        test.reduced.against.full.model of the <strong>Rmd file</strong>):</p>
      <p class="rcmd">anova(models.list$signif.pred, models.list$all.var)</p>
      <p>to get this:</p>
      <p class="rout">Analysis of Variance Table<br>
        <br>
        Model 1: log.kcal ~ log.carb + log.fat + logit.water<br>
        Model 2: log.kcal ~ log.ash + log.carb + log.fiber + log.protein +
        log.fat + logit.water<br>
        &nbsp; Res.Df&nbsp;&nbsp;&nbsp; RSS Df Sum of
        Sq&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; F Pr(&gt;F)<br>
        1&nbsp;&nbsp;&nbsp; 138
        3.0595&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <br>2&nbsp;&nbsp;&nbsp; 135 3.0429&nbsp; 3&nbsp; 0.016554 0.2448 0.8649</p>
      <p>This output shows the two models being compared, with Model 1 being the model without log.ash, log.fiber, and log.protein,
        and Model 2 being the complete model that includes all of the variables.
        The difference in residual sums of squares (RSS) is the variation that
        is explained by the three predictors that were dropped - any variation
        they explained was added to the residual term when they were omitted, so
        the difference in RSS is a measure of what they collectively explained.
        The difference between the two RSS numbers is the Sum of Sq for the
        test. The difference in residual degrees of freedom (Res.Df) is the
        degrees of freedom for these three predictors, and is the DF for the
        test. The MS for these three dropped predictors is Sum of Sq/Df, or
        0.016554/3 = 0.005518, and the MS used for the error termis for the
        second (complete) model, which is 3.0429/135 = 0.02254. The F value is
        0.005518/0.02254 = 0.2448, which with a numerator DF of 3 and a
        denominator DF of 135 gives a p-value of 0.8649. This test tells us that
        these three predictors were dropped because they didn't explain much
        variation in log.kcal separate from the variables that were left in the
        model, not because they were statistically redundant with one another.</p>
      <p>The way this test was presented in your book, and in lecture, was to
        sum the degrees of freedom and the explained SS for the three predictors
        from a Type I table. To match what we get by comparing the models
        together we need to re-run the model with log.ash, log.protein, and
        log.fiber entered last. The Type I SS table for this model is:</p>
      <p class="rout">Analysis of Variance Table<br>
        <br>
        Response: log.kcal<br>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        Df Sum Sq Mean Sq&nbsp;&nbsp; F value Pr(&gt;F)&nbsp;&nbsp;&nbsp; <br>
        log.carb&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 95.714&nbsp; 95.714 4246.4281
        &lt;2e-16 ***<br>
        log.fat&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 13.655&nbsp; 13.655&nbsp;
        605.8355 &lt;2e-16 ***<br>
        logit.water&nbsp;&nbsp; 1&nbsp; 2.105&nbsp;&nbsp; 2.105&nbsp;&nbsp;
        93.3830 &lt;2e-16 ***<br>
        log.ash&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp; 0.010&nbsp;&nbsp;
        0.010&nbsp;&nbsp;&nbsp; 0.4241 0.5160&nbsp;&nbsp;&nbsp; <br>
        log.protein&nbsp;&nbsp; 1&nbsp; 0.001&nbsp;&nbsp;
        0.001&nbsp;&nbsp;&nbsp; 0.0405 0.8408&nbsp;&nbsp;&nbsp; <br>
        log.fiber&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp; 0.006&nbsp;&nbsp;
        0.006&nbsp;&nbsp;&nbsp; 0.2698 0.6043&nbsp;&nbsp;&nbsp; <br>
        Residuals&nbsp;&nbsp; 135&nbsp; 3.043&nbsp;&nbsp; 0.023</p>
      <p>The sum of the Sum Sq for the last three predictors is 0.010 + 0.001 +
        0.006 = 0.017, which is the same as our test of the two models against
        each other. To get the p-value for this test we would use the command
        pf(0.2448, 3, 135, lower.tail = F), which would give us the same p =
        0.8649 we got above. Since comparing the two models together can be done
        in a single command it's a simpler way to test for the effect of
        dropping three variables at once.</p>
      <p>The other thing we can look at to confirm that these predictors weren't
        adding much to the all.var model is to look at the un-adjusted R<sup>2</sup>
        for the all.var and signif.pred models - all.var had an r.squared of
        0.9734, and signif.pred had an r.squared of 0.97832, which is only
        slightly lower. Dropping the non-significant predictors barely altered
        the amount of variation explained. It's not surprising, then, that
        dropping them produced a better model.</p>
      <p>Finally, we have focused on which model is best from a set of
        candidates, but it is possible that the best model is not even
        statistically significant. With such high R<sup>2</sup> values this is
        not really a concern, but we can confirm our suspicion that the model is
        significant by looking at the summary output for the signif.pred model.
        Get the summary() for this model in chunk summary.of.signif.pred, and
        the overall model test is here:</p>
      <p class="rout">Residual standard error: 0.1489 on 138 degrees of freedom<br>
        Multiple R-squared:&nbsp; 0.9733,&nbsp;&nbsp;&nbsp; Adjusted
        R-squared:&nbsp; 0.9727 <br>
        F-statistic:&nbsp; 1676 on 3 and 138 DF,&nbsp; p-value: &lt; 2.2e-16</p>
      <p>You can see from the p-value that this is a statistically significant
        model.</p>
      <blockquote>
      <p>Note that in our model with the highest adjusted R<sup>2</sup>
        log.protein doesn't make the cut. This may be puzzling to you as
        Biologists, because this result seems to be telling us that protein
        doesn't have any calories. We know better than that - in fact,
        protein and carbohydrates have about the same amount on average, 4
        kcal/g. </p>
      <p>So, why isn't protein significant? The reason is that most of the
        variation in log.kcal that log.protein explains is also explained by
        log.fat, log.carb, and logit.water. Including protein in the model is
        thus statistically redundant, and log.protein isn't significant when it
        is included with these other variables.</p>
      <p>Which reminds us about the pitfalls of interpreting correlated predictors - just because a variable is confounded with others in the model doesn't mean it is unimportant! To tell for sure you would need a different data set
        with less correlation (ideally, zero correlation) between the
        predictors, which is something we will never get from a random selection
        of foods. To get zero correlation between predictors requires a designed
        experiment that varies these nutrients independently in a factorial
        design.</p></blockquote>
      <h2>Models as hypotheses</h2>
      <p>A linear model is a hypothesis about the structure of the response
        data. The variables we include, the scale we use (i.e. log or linear), and any interactions we include are part of the hypothesis, and we can think of different models as being competing hypotheses. We can use model selection to allow us to evaluate which
        hypothesis is best supported by the data. Specifically:</p>
      <ul>
        <li>Including a predictor variable in a model hypothesizes that it has
          an effect on the response</li>
        <ul>
        </ul>
        <li>Including an interaction between two variables hypothesizes that the
          response to one predictor depends on the level of another.</li>
        <li>Including a log-transformed predictor, or a quadratic or cubic term
          hypothesizes that response to that predictor variable is non-linear.</li>
        <li>For a categorical variable, the factor levels used are a hypothesis
          that a) all of the groups used are different from one another, and b) only the
          groups used are different from one another</li>
        <ul>
          <li>If we modify a categorical variable by combining factor levels
            together we hypothesize that the levels we combine are not different</li>
          <li>If we modify a categorical variable by splitting one factor level
            into two or more new groups we hypothesize that the grouping was
            too broad, and that there were differences between means of the
            split groups that needed to be accounted for</li>
        </ul>
        <li>And, conversely, if we leave out a predictor, an interaction, a
          non-linear relationships, or a grouping variable we are hypothesizing
          that those terms do not have an effect on the response.</li>
        <ul>
        </ul>
      </ul>
      <p>Using the adjusted R<sup>2</sup> to identify the best model can be interpreted as identifying the hypothesis that is best supported by the data. </p>
      <p>Note that we will develop this way of thinking about statistical
        analysis over the last few weeks of the semester as we start talking
        about likelihood-based model selection. Using adjusted R<sup>2</sup> in
        this way has some disadvantages over the likelihood-based methods we
        will learn next, but we will start getting used to using models as
        hypotheses today.</p>
      <h3>Hypothesize about food - it's what we eat</h3>
      <p>Before you started working with the data set today you probably already
        knew something about how macronutrients and other major components of
        food influence caloric content, and you may have learned a little more
        from the models you constructed. </p>
      <p>But, you don't eat protein, carbohydrate, fat, water, ash, and fiber - what you eat is food. If you found yourself stranded on a desert island,
        you would want to find the most calorically dense foods to help you
        survive while you waited to be rescued, and you wouldn't be able to
        chemically analyze the foods available. Instead, you would have to
        select food to eat based on observable characteristics.</p>
      <p>Your next task is to use observable characteristics of the foods to
        develop <em>two different</em> hypotheses about <strong>groups of
          foods that differ in average caloric content</strong>. Once you decide on a hypothesis, you will use it to group the foods based on it, in a new categorical variable. You will then use
        this categorical variable to predict log.kcal. </p>
<p>If your hypothesis is well supported by the data it will explain a lot of the variation in the data, with as few groups as possible. Adding groups that are not different consumes degrees of freedom without adding explained variation, which increases adjusted R<sup>2</sup> - on the other hand, failing to split groups that are very different will result in a simpler model, but the explained variation will be lower, which increases adjusted R<sup>2</sup>. The best model will be the one that explains as much variation as possible with the smallest number of groups possible.</p>
<h4>1. Develop your first hypothesis. </h4>
<h4></h4>
<p>Download <a href="macronutrients_nopredictors.xlsx">this version</a> of
        the data and open it in Excel. You will see that the name of the food is
        still present and visible. I've included an example to guide you, called        H.example. H.example represents the hypothesis that the different
        anatomical parts of a plant differ in their caloric contents. Having developed that hypothesis, I used the H.example column as a representation of my hypothesis by entering the plant part each food is derived from.</p>
<p>Note that your two hypotheses do not need to be related to my H.example in
        any way - you are free to develop your hypotheses based on any
        observable characteristic, including taste, sweetness, whether you like
        the food (maybe you only like to eat plants that are high in calories,
        or that are low in calories), the "culinary" designation of the food
        (vegetable, fruit...), and so on. There is no need for your categories
        to start with mine and lump them or split them, you can develop whatever
        hypothesis you like and use whatever categories are necessary to test
        your hypothesis.</p>
      <h4>2. Translate your hypotheses into variables in Excel file</h4>
<p>Once you have the hypotheses you want to test, translate your hypothesis into a categorical variable you can use as a
        predictor in a linear model, just like I did in H.example. Use H1 for your first set of hypothetical groups.</p>
<p>Then, do the same for your second hypothesis, using column H2. Save the Excel file.</p>
<p>Note that the log.kcal data are in the file, but the column is hidden so you can't use it in developing your hypotheses. It will be included when you import the Excel file into R.</p>
<p><em><strong>3. Evaluate which hypothesis is best supported by the data.</strong></em></p>
      <p>Import the data from macronutrients_nopredictors.xls into R, and put it into an object called hypotheses.</p>
<p>Now, make an empty model list for the three hypotheses you will compare
        (make.food.hypotheses.list chunk of your <strong>Rmd file</strong>): </p>
      <p class="rcmd">food.hypotheses.list &lt;- list()</p>
      <p>In R, fit one model for each hypothesis. For H.example the command is
        (make.H.example.model chunk of the <strong>Rmd file</strong>):</p>
      <p> </p>
      <p> </p>
      <p> </p>
      <p class="rcmd"> food.hypotheses.list$H.example &lt;- lm(log.kcal ~
        H.example, data = hypotheses)</p>
      <p>To run the models for your two hypotheses, you will need to make your
        own R code chunks. Rmd files are just text files, and the way that
        RStudio knows how to interpret various components of the file as R code
        is by using symbols that identify where the code chunks are. A code
        chunk in an Rmd file has to have an opening and closing set of
        back-ticks, and curly braces with the "r" code letter inside. You can
        type these in by hand, like so: </p>
      <p>```{r}</p>
      <p>```</p>
      <p>The back-ticks are on the same key as the tilde, ~, but without using
        the SHIFT key. I have been labeling the code chunks to make it easier to match the
        instructions with the Rmd file, but that isn't required. If you want to
        label your code chunks, just add a label after the r in your header -
        for example, to label the first one my.hypothesis.1 you would use {r
        my.hypothesis.1}.</p>
      <p>When you have entered this set of codes you'll have a working
        R code chunk that you can use to run the models for your two hypotheses. Run an lm() using H1 as a predictor for your first hypothesis, and H2
        as a predictor for the second - add each to your food.hypotheses.list, with labels H1 and H2, respectively.</p>
      <p>Then, make one last code chunk, and enter the commands from above that
        extract r.squared and adj.r.squared for the models in
        food.hypotheses.list. Pick whichever of your two models, either H1 or
        H2, has the higher adjusted R<sup>2</sup>, and report its results to the
        class database on our cc site. Specifically, you will enter:</p>
      <ul>
        <li>A statement of your hypothesis (mine would be "plant parts differ in
          caloric content").</li>
        <li>The adjusted R<sup>2</sup></li>
        <li>The categories you used. You can get these quickly by making your
          hypothesis columns into factors, and then getting the levels - for
          H.example the command would be levels(factor(hypotheses$H.example))</li>
      </ul>
      <p>Once everyone has recorded their results we will look at which
        hypothesis has the highest adjusted R<sup>2</sup>. See if you can beat
        mine, it's pretty good - I've already entered it into the database, you
        only need to report the best of your two hypotheses. We'll look over the
        results when we meet the week after the exam.</p>
      <p><br>
      </p>
      <p> </p>
      <p>That's it! Knit and upload your Word file to complete the assignment. </p>
    </div>
</body></html>