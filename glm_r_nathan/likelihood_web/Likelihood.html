<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <title>Likelihood</title>
    <link href="https://csusm-my.sharepoint.com/personal/wkristan_csusm_edu/_layouts/15/guestaccess.aspx?docid=02a5532ccf4574e49ae6e245b4a118252&amp;authkey=AcYXNyrvFGWnmf6cbl1qQQ4&amp;e=4170fc6099b74d3f94a7fdaade97010e"
      rel="stylesheet" type="text/css">
    <script src="https://csusm-my.sharepoint.com/personal/wkristan_csusm_edu/_layouts/15/guestaccess.aspx?docid=04e5dfc4b7ee64a7dbacb487cddde06a0&authkey=AdStuSX7RsXlg4QXZtzjDfw&e=f5861611192540a4b0c28483e9e5baf0"></script>
  </head>
  <body>
    <div id="header">
      <div style="float: left"><button onmouseover="navToggle()">☰</button></div>
      <h1>Likelihood</h1>
    </div>
    <div id="navigation" style="display:none" onclick="navToggle()"> 
        <p><a href="#intro">Introduction</a></p>
        <p><a href="#analytical">ML analytical solutions</a></p>
        <p><a href="#numerical">ML numerical solutions</a></p>
        <p><a href="#mle">Numerical ML estimate of the mean</a></p>
        <p><a href="#mle_slope">ML estimate of slope</a></p>     
        <p><a href="#hyp_test">Hypothesis testing</a></p>
        <p><a href="#mle_int_only">Intercept only models</a></p>     
        
    
    
    </div>
    <div id="content">
      <p class="part" id="intro"> We will be making extensive use of likelihood
        for the last activity of the semester, and to prepare you this exercise
        will give you some hands-on experience working with likelihoods. The
        examples we use today are likelihood approaches to doing things you
        already know how to do, like estimating a mean and fitting a regression
        line to some data. Next week we will use the basic approach you learn
        today to do likelihood-based model selection.</p>
      <p>We will work today with Excel, so that you can calculate some
        likelihoods by hand and get a better idea how they work. We will be
        using numerical approaches that give approximate values of maximum
        likelihood estimates. It isn't always necessary to find approximate
        solutions, so as an aside let's see how a likelihood function can be
        used to find an exact maximum likelihood estimate - that is, an
        analytical solution.</p>
      <h2 class="part" id="analytical">Analytical solutions with likelihoods</h2>
      <p>In lecture, we looked at the basic conceptual basis for finding maximum
        likelihood estimates - we used a likelihood function to calculate the
        likelihoods of a range of possible values for the parameter we are
        estimating, and then we picked the possible value with the greatest
        likelihood given the data as our parameter estimate. In many cases it's
        possible to derive analytical formulas that give exact maximum
        likelihood estimates, instead of approximations. You will see that the
        approximations we use are very good, but to give you an idea of how an
        analytical approach would work we will find the maximum likelihood
        estimator for the population mean, given a collection of data with
        values 119.42, 123.67, 124.86, 125.17, 125.3, 126.9, 126.96, 128.13,
        128.31, 128.74, 130.36, 130.63, 130.78, 132.53, 135.6</p>
      <p> </p>
      <p> </p>
      <p> </p>
      <table width="100%" border="0">
        <tbody>
          <tr>
            <td><img alt="Mean" src="mean.png"></td>
            <td>
              <p>We have an analytical formula for a sample estimate of the
                population mean (μ), but how do we know it's the right formula?
                To know if it's the best possible estimator for the population
                mean we need a criterion for "best".</p>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" border="0">
        <tbody>
          <tr>
            <td><img alt="Normal log likelihood" src="normal_log_likelihood.png"></td>
            <td>
              <p>If we define "best" as being the estimate for μ that has the
                highest likelihood given the data, we can find the value of μ
                that maximizes the log of the likelihood function, and use that
                as our estimate of μ - the <strong>maximum likelihood estimate</strong>.
                If you remember from your calculus class, you find a maximum of
                a function of a variable by taking the first derivative with
                respect to the variable, setting the first derivative equal to
                0, and then solving for the value of the variable. The
                log-likelihood function, assuming the data are normally
                distributed, is shown to the left. The maximum likelihood
                estimate for μ (which is symbolized with a "hat" over it to
                indicate it's an estimate) is the value of μ that maximizes this
                function. </p>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" border="0">
        <tbody>
          <tr>
            <td><img alt="Normal likelihood" src="norm_likelihood_mu.png"><br>
            </td>
            <td>
              <p>The log likelihood function for μ given the data values is
                graphed to the left in black. Slopes of tangent lines to the
                curve give the rate of change in the log likelihood with a
                change in&nbsp;μ. The red line is tangent to the log likelihood
                at the curve's maximum, and you can see the line is flat, with a
                slope of 0. This means we can find the maximum of the log
                likelihood by finding where the slope of a tangent line is equal
                to 0.</p>
              <p>Slopes of tangent lines to curves are found by finding the
                first derivative of function. Setting the first derivative to 0
                allows us to solve for the value of the variable at the point on
                the curve where the tangent line has a slope of 0 - that is, we
                can find the value of the variable at the maximum of the curve.</p>
              <p>Setting the first derivative of the log likelihood to zero and
                solving for μ gives us a formula, which we can use as the <strong>maximum
                  likelihood estimator</strong> for μ. </p>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" border="0">
        <tbody>
          <tr>
            <td><img alt="First derivative" src="first_deriv.png"></td>
            <td>
              <p>We actually need to use partial derivatives because there is
                another parameter in the function, σ; partial derivatives treat
                variables other than the one we are solving for as though they
                are constants, so we will find the derivative in the usual way,
                using the polynomial rule for finding derivatives, treating σ as
                a constant. Treating σ as a constant means that the first two
                terms are constants that are dropped from the first derivative,
                and what's left is set to 0.</p>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" border="0">
        <tbody>
          <tr>
            <td><img alt="Sum equal zero" src="sum_equal_zero.png"></td>
            <td>
              <p>Notice that we can only make the equation equal 0 if the Σ(x<sub>i</sub>
                - μ) part is equal 0, so we can focus on that portion. Since μ
                is repeated for each residual calculation within the summation,
                we can re-express the summation as the sum of the data minus n
                times μ.</p>
            </td>
          </tr>
          <tr>
            <td><img alt="Esimate" src="estimate.png"></td>
            <td>
              <p>Solving for μ gives us the familiar formula for <span style="text-decoration: overline">x</span>,
                which tells us that when we calculate <span style="text-decoration: overline">x</span>
                we are calculating the maximum likelihood estimate of&nbsp;μ
                given the data, provided that the data are normally distributed
                like our likelihood function assumes them to be.</p>
            </td>
          </tr>
        </tbody>
      </table>
      <p>Likelihood functions don't always have analytical solutions like this
        one. When analytical solutions are not available it's possible to find
        solutions by trying out different possible values for the parameter
        until the smallest possible value of the likelihood function is
        achieved; solutions found by trial and error are called <strong>numerical
          solutions</strong>. </p>
      <h2 class="part" id="numerical">Numerical solutions for likelihood
        functions</h2>
      <p>A numerical solution starts with a guess of the value of the parameter
        to be found, and then tries out a different value above or below the
        initial one and checks whether the new value improves on the initial
        guess - if so, the searching in that direction until new guesses stop
        improving on old ones. Computers are very good at doing this kind of
        work, and numerical methods usually arrive at correct solutions very
        quickly.</p>
      <p>Numerical methods can only give approximate solutions to a fixed level
        of precision, but this is not really a problem. When we calculate the
        mean for a data set using Σx/n we are calculating an <strong>analytical
          solution</strong>, meaning that it is mathematically correct. With an
        analytical solution we can calculate an infinite number of decimal
        places and it will be correct (it's correct to an <em>arbitrary</em>
        level of precision, meaning it's up to us how many decimal places to
        use). However, this is only true in the abstract, because experimental
        data does not have infinite precision. Our data values are only measured
        to 2 decimal places, and an analytical solution calculated beyond that
        level would only still be correct if digits beyond the second decimal
        place were all 0. So, when we calculate statistical estimates from our
        data we round the estimates to reflect the level of precision of our
        measurements. So, as long as our numerical estimates are correct to the
        number of significant digits we are reporting they can be considered
        just as accurate as analytical solutions.</p>
      <p>However, numerical method can fail to work at all or (worse) can seem
        to work but give incorrect solutions, so you should be aware of
        conditions that can cause problems with numerical methods. Two cases you
        may encounter are:</p>
      <ul>
        <li>Numerical solutions can get stuck in a local maximum</li>
        <li>Numerical solutions may fail to find a solution at all if the
          likelihood function is too flat near the maximum</li>
      </ul>
      <p>The first problem could conceivably cause you trouble at some point,
        but only if you are working with complex likelihood functions. The
        likelihood functions we work with today will be nice, smooth curves that
        only have one maximum value, and we will get solutions easily.</p>
      <table width="100%" border="0">
        <tbody>
          <tr>
            <td><img alt="Local minima" src="local_mimima.jpg" vspace="5" hspace="50"
                align="left"></td>
            <td>
              <p>But, just for the sake of illustration, consider the example to
                the left. This graph shows a complex negative log-likelihood
                function with more than one minimum (we want the maximum
                likelihood, so we are looking for the minimum of the <em>negative</em>
                log-likelihood). The <strong>global minimum</strong> at G is
                the lowest point the function achieves, and should be the
                estimate of μ. However, if we started searching for μ at A' at
                the left edge of the graph and worked to the right, we could
                arrive at a <strong>local minimum</strong> at A and think we
                had found the maximum likelihood estimate. We would only find
                the global minimum at G if we started searching close enough to
                G to find it. </p>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" border="0">
        <tbody>
          <tr>
            <td colspan="2" rowspan="1">
              <p>Avoiding local minima in numerical work is done by a) using
                starting values as close to the global minimum as possible, and
                b) using several different starting points to make sure that the
                same minimum is always found. The likelihood function we're
                working with today only has one minimum anywhere near the
                maximum likelihood estimates, so we won't have a problem. Just
                be aware of it if you go on to bigger and better things once
                you're done with this class.</p>
              <p>The second problem, lack of convergence on a solution, is due
                to the fact that we're using computers to find our estimates.
                Computers can only store a fixed number of decimal places for a
                continuous number, usually 15 decimal places. We've seen this
                before in R - it uses double precision floating point
                representation of decimal numbers, and the smallest value it can
                represent for continuous numbers is 2.2e-16. If you have a
                likelihood function that is so flat at the maximum value that
                values around it have likelihoods that are identical to 15
                decimal places, the computer won't be able to tell them apart,
                and the function will fail to converge on a solution and will
                give you an error.</p>
            </td>
          </tr>
          <tr>
            <td style="width: 342px;"><img style="width: 302px; height: 232px;"
                alt="Log reg" src="lr_converge.jpg" hspace="50 vspace = " align="left"></td>
            <td style="width: 695px;">
              <p>The problem is easiest to see if we use a non-linear function,
                like the logistic curve to the left. The logistic curve is often
                used to predict the probability of an outcome based on a
                predictor variable (for the example, we might predict the
                probability of catching a cold based on the number of viral
                particles you're exposed to, where x is the number of viral
                particles). The red line is the predicted probability, and the
                blue squares are the data points - the data are coded with a 1
                indicating that the patient has a cold, and 0 indicates that
                they do not.</p>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" border="0">
        <tbody>
          <tr>
            <td><img style="width: 361px; height: 300px;" alt="Converged" src="likelihood_converge.jpg"></td>
            <td>
              <p>The example above shows good separation between people with or
                without a cold, but there is some overlap in the middle where we
                transition from a very low probability of getting a cold to a
                very high probability of getting a cold. This region of overlap
                has the biggest effect on the likelihood function, because it's
                where the residuals are biggest, and a change in the estimate of
                the slope causes the biggest change in the residuals in this
                middle region. </p>
              <p>The negative log-likelihood function is shown the left, and
                thanks to the small amount of overlap in the data the likelihood
                function has a definite minimum, which the numerical approach
                would have no trouble finding.</p>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" border="0">
        <tbody>
          <tr>
            <td><img alt="diff slopes" src="lr_no_converge.jpg"></td>
            <td>
              <p>Now consider a case where we didn't get any measurements in
                this crucial part of the function, between 4 and 6 where the
                curve transitions from predicting values near 0 and values near
                1. The separation between the infected (1) and uninfected (0)
                patients is perfect, so it doesn't seem as though there should
                be a problem - we couldn't hope for a clearer result.</p>
              <p>But, since there are no measurements between x of 4 and 6 where
                the shape of the curve is actually determined, multiple logistic
                curves fit the data nearly equally well. The three curves shown
                here are three out of an infinite number with infinitesimally
                small differences in slope and intercept that can be used to
                describe the data. Provided that the curves are very flat by the
                time they encounter the data they will all explain the data very
                well.</p>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" border="0">
        <tbody>
          <tr>
            <td><img style="width: 360px; height: 300px;" alt="No convergence" src="likelihood_no_converge.jpg"></td>
            <td>
              <p>Because different possible values for the slope explain the
                data very well, the negative log likelihood function is nearly
                flat across a broad section. There is still a value that
                minimizes this function, but it would be different from the
                values near it by a very small amount. If the likelihood in the
                vicinity of the minimum are the same as the minimum to 15
                decimal places, then the computer can't tell which is the best
                value. If you ever get an error message telling you that you
                "failed to converge" on a solution, this is a likely culprit.</p>
              <p>How to fix this? It would be necessary to measure some values
                along the x-axis that are currently unmeasured, between about 4
                and 6, so that the best curve can be evaluated.</p>
            </td>
          </tr>
        </tbody>
      </table>
      <p>Fortunately for us, general linear models tend not to have the
        difficulties mentioned above, and we definitely won't have any trouble
        getting our estimates today.</p>
      <h2 class="part" id="mle">Maximum likelihood estimation - estimating the
        mean for a set of data</h2>
      <p> </p>
      <p>We are going to take a break from R today (and rejoicing was heard
        throughout the land) and will use Excel instead. R is a much better
        platform for actually getting this kind of work done, and we'll go back
        to it in the next activity, but doing the calculations in Excel is a
        good way to learn what likelihoods are actually about.</p>
      <p>We will be using Excel's tool for finding numerical solutions, called
        the Solver. Solver needs several things to work properly:</p>
      <ul>
        <li>An objective cell - this is a single cell in the spreadsheet
          containing a spreadsheet formula that depends on one or more sets of
          values we wish to estimate. We also need to tell Solver whether we
          want the value of the objective cell to be:</li>
        <ul>
          <li>Maximized - in which case values of the estimates will be found
            that make the objective as big as possible.</li>
          <li>Minimized - in which case values of the estimates will be found
            that make the objective as small as possible.</li>
          <li>Set to a specified target value - in which case values of the
            estimates will be found that make the objective equal the value we
            specify.</li>
        </ul>
        <li>One or more cells to change. These are the cells that contain the
          parameters we want to estimate. Our parameters have to be used,
          directly or indirectly, in the formula for the objective cell. We can
          specify as many cells to change as we want.</li>
      </ul>
      <p>The instructions below assume that you know that Excel can be used to
        do calculations, that formulas start with an = sign, that cells can be
        referenced by row and column pair (i.e. A1 means column A row 1), and
        that dollar signs before either the column or row identifier makes it an
        "absolute" reference that stays the same when you copy/paste the cell.
        If any of these things are new to you and you need some additional
        explanation, please let me know and I'll be glad to help you.<br>
      </p>
      <p>The spreadsheet you'll need for this exercise is <a href="likelihood_data.xls">here</a>.
        Download it to your computer and open it in Excel.</p>
      <p> </p>
      <p> In the worksheet EstimateMean, you'll find 15 data points in column A,
        labeled "Data". We'll start by setting up this worksheet to calculate
        the likelihood of each data point, given an initial guess at the
        estimate of the mean of 127 and of the standard deviation of 3.95.</p>
      <p>1. Set up some column labels, and enter the initial guesses for mean
        and standard deviation, like so:<br>
      </p>
      <p> </p>
      <p> </p>
      <ul>
        <li>
          <p>In the first row of column B, type "Likelihood". </p>
        </li>
        <li>
          <p>In cell A19 type the word "Mean", and in B19 type 127. </p>
        </li>
        <li>
          <p>In cell A20 type the word "StDev", and in cell B20 type "3.95".
            Your sheet should look like <a href="individual_likelihood1.png">this</a>.</p>
        </li>
      </ul>
      <p> </p>
      <p> </p>
      <p> 2. Calculate the likelihoods of the first data point. Into cell B2
        type the function:</p>
      <p><span class="Excel">=normdist(a2, b$19, b$20, 0)</span></p>
      <p> </p>
      <p> This function gives the normal probability for a number (a2) with a
        specified mean (b$19) and standard deviation (b$20). The last argument,
        0, indicates that this is not a cumulative probability, but rather a
        probability density at the data value specified.</p>
      <p>Recall that we can use probability distributions as likelihood
        functions, so even though the normdist() function is calculating
        probability densities, we can use it as a likelihood function instead -
        cell B2 is thus the likelihood of a mean of 127 given the known data
        value of 119.42.</p>
      <p> </p>
      <p> 3. Now you can calculate the likelihoods of the remaining data points.
        Copy cell B2, and paste it to B3 through B16. Since the formula used
        absolute cell references (i.e. dollar signs before the row numbers) for
        the mean (in B$19) and standard deviation in (B$20) but used a relative
        reference (A2) for the data value, each formula in B2 through B16 uses
        the same mean and standard deviation, but points to the data value in
        its own row. Consequently, you now have normal likelihoods for each of
        the 15 observations.</p>
      <p> </p>
      <p> As you scan through the likelihoods you just calculated, you'll see
        that some are bigger than others. Data values that are near 127 give
        high likelihoods, while values far from 127 give low ones. </p>
      <p> </p>
      <p> 4. You can now calculate the likelihood of the mean and standard
        deviation given all 15 of the data points. In cell A22 type "Overall
        likelihood", and in B22 type: </p>
      <p> </p>
      <p><span class="Excel"> =product(b2:b16)</span></p>
      <p> </p>
      <p> Likelihoods are combined by multiplying them, and the function
        "product" multiplies all the values from B2-B16 together. Since the
        individual likelihoods are values between 0 and 1, the largest of which
        is 0.1, the product across these 15 values is very small (on the order
        of 10<sup>-19</sup>). Make a note of the mean and its likelihood in your
        worksheet.</p>
      <p> </p>
      <p> 5. To get a feeling for how the maximum likelihood will be found,
        change the value of the mean in cell b$19 to other possible values. </p>
      <ul>
        <li>
          <p>Set the mean in b$19 to 119.42, which is the value of the first
            data point (they are sorted in order, so this is also the smallest
            number). You'll see the likelihood of this possible value of the
            mean is high judged by the first data point, but for most of the
            rest of the data points it is less likely than 127 was, and you'll
            see that the overall likelihood in cell B22 gets smaller. </p>
        </li>
        <li>
          <p>Set the mean to 135.61 (the value of the largest data point), and
            you'll see the likelihoods change again, with the last data point
            indicating high likelihood. As with the value of 119.42, only a
            couple of data points give a higher likelihood to 135.61 than to
            127, and the overall likelihood given the entire data set is lower
            than for the value of 127.</p>
        </li>
      </ul>
      <p> </p>
      <p> </p>
      <p> </p>
      <p> 6. Label column C "-LogLikelihood" (you'll need the quotes or Excel
        will think the negative sign is part of a formula and give you an error
        message), and in cell C2 type:</p>
      <p> </p>
      <p><span class="Excel"> =-ln(b2)</span></p>
      <p> </p>
      <p> </p>
      <p> </p>
      <p> You can now copy and paste this equation to cells C3-C16. </p>
      <p>Multiplying across raw likelihoods is equivalent to adding across
        log-likelihoods, so the overall -LogLikelihood is just the sum of the
        individual -LogLikelihoods. In cell A23 write "Overall -LogLikelihood",
        and in cell B23 type:</p>
      <p> </p>
      <p><span class="Excel"> =sum(C2:C16).</span></p>
      <p> </p>
      <p> </p>
      <p> 7. Now it's time to find the best value for the mean. So far we have
        just been plugging in arbitrary values for the mean, but we can find the
        best possible value using Excel's Solver plug-in. Solver is used to find
        numerical solutions, which is what we want.</p>
      <p> </p>
      <p> </p>
      <p> You may need to turn Solver on before you can use it. Go to the "File"
        tab in the upper left corner of Excel, and select the "Options" near the
        bottom of the list on the left. In the window that pops up, select the
        "Add-Ins" option out of the list, and click on the "Go..." button next
        to "Manage: Excel Add-ins". Click on the check-box next to the "Solver
        Add-in", and click "OK". If you now click on the "Data" tab, you should
        see "Solver" as one of the data analysis options (right hand side of the
        toolbar).</p>
      <p> </p>
      <p> </p>
      <p> </p>
      <p>To run Solver:</p>
      <ul>
        <li>Click the "Solver" button in the "Data" tab.</li>
        <li>Use the following settings:</li>
        <ul>
          <li>Enter $B$23 into the "Set Objective" box - this is the negative
            log likelihood.</li>
          <li>Click on the "Min" option so that we will minimize the negative
            log-likelihood in cell B23 (minimizing the negative log-likelihood
            maximizes the likelihood).</li>
          <li>Enter $B$19 as "By Changing Cells:". We won't estimate the
            standard deviation this time, keep it at 3.95.</li>
          <li>Check the box for "Make unconstrained variables non-negative".
            This means that if we don't set a constraint that tells Solver what
            ranges of values to consider it should still only consider
            non-negative numbers. We don't need to consider a negative value for
            the mean, if the data can't possibly have negative values.</li>
          <li>You are now set to find the value of the mean in B19 that makes
            the sum of the negative log-likelihoods as small as possible (it
            should look like <a href="solver.png">this</a>). </li>
        </ul>
        <li>Click "Solve" to run. When Solver has found a solution, select "OK"
          to keep it.</li>
      </ul>
      <p> </p>
      <p> For comparison, in cell C19 type =average(A2:A16). The analytical
        solution provided by average() should match Solver's estimate out to six
        or more decimal places. With only two decimal points recorded in the
        data, if they are the same to three decimal points then they are
        identical at the level of precision you require. </p>
      <p>The take-home message being, numerically minimizing a negative
        log-likelihood to find an estimate for a parameter works.</p>
      <p> </p>
      <p> Now that you have a basic idea of what likelihoods are, and how they
        can be used, we will use likelihood to find the best-fit line through a
        data set.</p>
      <p> </p>
      <h2 class="part" id="mle_slope">Maximum likelihood estimation - estimating
        slope and intercept of a regression line</h2>
      <p> The next step is to use this approach to estimate the slope (β) and
        intercept (α) of a linear regression model. The log likelihood function
        looks like this:</p>
      <p> </p>
      <img alt="Log likelihood" src="log_like_function.png">
      <p> </p>
      <p> The cursive L is the symbol for likelihood. The parameters we are
        estimating are on the left side of the vertical line, meaning they are
        unknowns, and the data are on the right side, meaning it's known. Notice
        that two of the parameters we want to estimate, β and&nbsp;α, don't seem
        to be in the likelihood function. This is because we won't be maximizing
        the likelihood of β or α individually - instead, we will be maximizing
        the likelihood of the linear regression model as a whole, by varying β
        and&nbsp;α. On the far right right side of the function you'll see that
        the predicted values from the line (ŷ) are being subtracted from the
        data values (y<sub>i</sub>); observed minus predicted values are
        residuals. The predicted values (ŷ) come from the regression equation,
        so if we wanted to show more clearly how β and&nbsp;α are involved in
        the likelihood function we could write the ŷ as βx + α, so that the
        residuals would become (y<sub>i</sub> - (βx + α)). As we vary the
        possible values of slope and intercept the residuals will change, which
        in turn will change the likelihood. Values for slope and intercept that
        maximize the likelihood across the full set of data will be our
        estimates of β and&nbsp;α. The standard deviation will be varied as
        well, so we will also get an estimate of the standard deviation of the
        residuals.</p>
      <p> </p>
      <p> 1. Switch to the sheet "Regression". You'll see a set of data, labeled
        y and x. I've labeled columns C for "Predicted values", D for
        "Likelihood", and E for "-LogLikelihood". Below, in rows 20-23 of Column
        B are initial values for the slope, intercept, and standard deviation of
        the residuals. We don't have a mean this time, because we are going to
        use the regression equation to predict the mean at each x value using
        the slope and intercept. </p>
      <p>2. To calculate the predicted values, type into C2:</p>
      <p> </p>
      <p><span class="Excel"> =b$20*b2 + b$21</span></p>
      <p> </p>
      <p> Copy and paste this formula to C3 through C17. These are the predicted
        means for y from a straight line with a slope of 1 and an intercept of
        8.755. You'll see the predictions are terrible - never fear, we know
        these are not good estimates, we just need to specify starting values to
        give the Solver something to modify.</p>
      <p> </p>
      <p> 3. To calculate the likelihoods for each data point, type into D2: </p>
      <p> </p>
      <p><span class="Excel"> =normdist(a2, c2, b$23, 0)</span></p>
      <p> </p>
      <p> This gives the likelihood for a mean equal to the predicted value (in
        c2) given the data value (in a2), with a standard deviation of the
        residuals specified in b$23. We didn't need to calculate a residual
        because normdist() does it for us - it uses the difference between the
        data value in a2 and the mean in c2 (i.e. the residual) to calculate a
        normal probability.</p>
      <p>Copy and paste this formula from D2 to D3 through D17. </p>
      <p>4. Now, convert the likelihoods to negative log-likelihoods. In cell E2
        type:</p>
      <p> </p>
      <p><span class="Excel"> =-ln(d2)</span></p>
      <p> </p>
      <p> Make sure you include the negative sign before the ln(). Copy this
        into E3 through E17. </p>
      <p>To get the negative log-likelihoods for the model given the entire set
        of data, we just need to sum these values - in cell B25 type:</p>
      <p> </p>
      <p><span class="Excel"> =sum(e2:e17)</span></p>
      <p> </p>
      <p> 5. You can now use Solver to find the best values for the slope,
        intercept, and standard deviation of the residuals. </p>
      <p>Start up the Solver, and use the settings:</p>
      <ul>
        <li> Use B25 as the objective (the sum of the negative log likelihoods)</li>
        <li>Minimize the objective cell</li>
        <li>Use B20, B21, and B23 as the cells to change. </li>
        <li>Do not make unconstrained variables non-negative.</li>
        <li>The Solver should look like <a href="solver_regression.png">this</a>.
          Click "Solve", and then accept the solution.</li>
      </ul>
      <p>You can compare the solutions to what you would get by fitting a linear
        regression by typing into C20 and C21:</p>
      <p> </p>
      <p><span class="Excel"> =slope(a2:a17, b2:b17)</span></p>
      <p><span class="Excel"> =intercept(a2:a17, b2:b17)</span></p>
      <p> </p>
      <p> These are the usual estimates of slope and intercept for a linear
        regression based on analytical formulas, and they should match the
        maximum likelihood estimates...this means that the analytical formulas
        for slope and intercept are the maximum likelihood estimates.</p>
      <p> </p>
      <p>Stop and think for a moment about what we did - we were able to get
        accurate estimates of slope and intercept by maximizing the likelihood
        of the model given the data. We were able to get these estimates without
        specifying a formula for slope or a formula for intercept, and without
        having to specify a separate likelihood function for each parameter we
        estimated. Maximum likelihood is a popular method for estimating
        parameters of a model fitted to data because of this.</p>
      <h2 class="part" id="hyp_test">Hypothesis testing with likelihood -
        comparing two nested models with a likelihood ratio test</h2>
      <p>The other major application of likelihood you will work with now is
        hypothesis testing. Although the goal is to obtain a p-value for a
        hypothesis, we will be using <em>models</em> as hypotheses rather than
        some null value for a parameter. Our focus will be on whether there is a
        statistically significant change in the likelihood of a model given the
        data when we drop a term from the model - if so, we would have evidence
        that the more complex model with the term included is a better
        hypothesis about the nature of the relationship between the predictors
        and the response than is the less complex model that omits the term.</p>
      <p> To use this approach, we need to fit two different models to the data
        and then compare their likelihoods. We have a simple linear regression
        now, so we will fit a more complex model, with a quadratic term, and see
        if this significantly improves the fit of the model to the data. To use
        this approach we need to use the same set of y data for both models, and
        the more complex model has to have everything that's included in the
        simpler model - that is, the simpler model has to be <strong>nested</strong>
        within the more complex model.</p>
      <p>1. Switch to the "Quadratic" tab. You'll see that the data are the same
        as on the "Regression" tab, and the layout of the column names and
        coefficients is very similar, except for a couple of things: </p>
      <ul>
        <li>Now we have a coefficient labeled "Linear" (instead of "Slope") to
          represent the linear term.</li>
        <li>We have an additional coefficient for the quadratic term (that is,
          the value multiplied by x<sup>2</sup>) labeled "Quadratic". </li>
        <li>The intercept is still labeled "Intercept", and the standard
          deviation of the residuals is still "StDev of residuals". </li>
      </ul>
      <p>We will have Solver minimize the negative log likelihood by changing
        all three of the model coefficients in B20 through B22, as well as the
        standard deviation of the residuals in cell B24.</p>
      <p> </p>
      <p>To make the task of finding the estimates easier for Solver, we're
        starting with the slope, intercept, and standard deviations of the
        residuals set to the values Solver found for the simple linear
        regression - since these values put the line through the data, starting
        with them will get us close to the final solution. We can use an initial
        guess for the quadratic term of 0, which means that initially the
        predicted values will be the same as the Regression tab - if Solver
        finds changing the quadratic coefficient improves the fit, then the
        predicted values will change.</p>
      <p>Calculate the predicted values (starting in cell c2) using the formula:</p>
      <p><span class="Excel">=B$21*B2^2+B$20*B2+B$22</span></p>
      <p>This adds a squared (quadratic) term to the predicted value, using the
        coefficient in b21 multiplied by b2 squared.</p>
      <p>Once you have predicted values for every data value, you can calculate
        the likelihoods, and the -log-likelihoods as before. Put the sum of the
        negative log-likelihoods in cell B26, and you're ready to use Solver to
        estimate the coefficients.</p>
      <p>2. Use Solver with the settings:</p>
      <ul>
        <li>The negative log-likelihood in B26 is the objective cell, and it
          should be minimized.</li>
        <li>Changing the values of B20:B22, B24. </li>
        <li>Un-check the box that says "Make unconstrained variables
          non-negative" - we want Solver to be able to try out negative
          coefficients.</li>
      </ul>
      <p>3. Now you have negative log-likelihoods for two nested models, so we
        can test for a difference between them using a likelihood ratio test.</p>
      <p>In cell A28 enter "Test of quadratic vs. linear".</p>
      <p>In cell A30 of the Quadratic sheet, type "Chisquare", and in B30 type:</p>
      <p><span class="Excel">=2*(Regression!B25 - Quadratic!B26)</span></p>
      <p>This will give you twice the differences in the negative
        log-likelihoods, which is the Chi-square test statistic. Chi-square
        values have to be positive, so if you do the subtraction out of order,
        take the negative of this difference as your Chi-square statistic.</p>
      <p>In cell A31 type "p", and in B31 type:</p>
      <p><span class="Excel">=chidist(B30,1)</span></p>
      <p>This function gives probabilities from the Chi-square distribution. The
        first argument is the Chi-square test statistic you calculated in cell
        B30, and the second argument it the degrees of freedom. The quadratic
        model has three coefficients and the regression model has two, for a
        difference of 1, so the degrees of freedom is 1.</p>
      <p>You'll see that adding the quadratic term didn't give us a significant
        change in the log-likelihoods of the models, so we wouldn't consider the
        quadratic term to be significant.</p>
      <h2 class="part" id="mle_int_only">Is the best-supported hypothesis any
        good at all?</h2>
      <p>Note that our likelihood ratio test gave us a comparison of two models,
        simple linear regression and quadratic regression, that don't differ
        significantly. The ratio of likelihoods for two models is a measure
        called <strong>support</strong> - if we put the more complex model in
        the numerator we are measuring how much more support there is in the
        data for the more complex model relative to the less complex model. We
        could generate a Chi-square test of significance by using negative twice
        the likelihood ratio as a Chi-square test statistic. The likelihood
        ratio test you just completed told you that there is not enough support
        in the data for adding a quadratic term to the model, and with that
        result you would prefer the linear model as the best one for your data.</p>
      <p>But, the likelihood ratio test bypassed the step of testing if the
        simple linear regression is significant - that is, our likelihood ratio
        test told us that there isn't support for a quadratic term to include
        it, but it didn't tell us if there is support for the notion that y
        depends on x in the first place.</p>
      <table width="100%" border="0">
        <tbody>
          <tr>
            <td><img alt="Intercept only" src="int_only.png"><br>
            </td>
            <td>
              <p>A model-based approach to asking this question is to fit an <strong>intercept
                  only</strong> model to the data. A model that only has an
                intercept is hypothesizing no relationship between y and x at
                all - or, if you prefer, it hypothesizes a flat line with a
                slope of 0 - in which case the intercept is equal to the mean of
                the y data. If we calculate the likelihood of this intercept
                only model, then we could compare it to the linear regression
                model, and if there is a significant difference in likelihood we
                could conclude that there is support for the hypothesis that y
                depends on x (i.e. we need the slope to explain the data).</p>
            </td>
          </tr>
        </tbody>
      </table>
      <p>1. In the Quadratic sheet, enter "Intercept only -LogLik" into cell
        A33.</p>
      <p>In cell B33 enter the formula (note the negative sign in front of the
        sum):</p>
      <p><span class="Excel">=-sum(ln(normdist(a2:a17, average(a2:a17),
          stdev(a2:a17), 0)))</span></p>
      <p>and hold down CTRL+SHIFT as you hit ENTER to make it an array formula
        (if you did it right you should get a value of 18.34317..). If you get
        an error message, #VALUE, then click into formula bar to get into edit
        mode and try again. </p>
      <p>Array formulas can apply an Excel function to a range of cells, one at
        a time. This formula is calculating the normal probabilities for the
        data in A2 through A17 one at a time, using the average and standard
        deviation of the data in those cells, calculating the natural log of
        each, summing them, and then multiplying by -1. This is thus the
        negative log-likelihood of a model using only the mean of the y data.
        Note that we're using average() and stdev() instead of using Solver to
        estimate these parameters - we know that the mean is a maximum
        likelihood estimator, so to save some time we'll take this shortcut.</p>
      <p>2. In cell A35 enter "Test of linear vs. intercept only". To get the
        likelihood ratio test for linear regression vs. intercept only:</p>
      <ul>
        <li>In cell A37 enter "Chisquare".&nbsp;</li>
        <li>In B37 enter <span class="Excel">=2*(Quadratic!B33-Regression!B25)</span>.
          This is the Chi-square test statistic for the difference in
          likelihoods between linear and intercept only models.</li>
        <li>In cell A38 enter "p", and in B38 enter <span class="Excel">=chidist(b37,
            1)</span>. Like the comparison of linear to quadratic regression,
          the intercept only model has one fewer coefficients than the linear
          regression (i.e. no slope for intercept only), so the degrees of
          freedom is 1.</li>
      </ul>
      <p>You'll see that the p-value is less than 0.05 - which means that
        including a slope in the model is well supported, as we get a
        significantly higher likelihood for a model that includes a slope than
        one that does not.<br>
      </p>
      <p> </p>
      <p> For your assignment, upload the spreadsheet to Cougar Courses.</p>
      <br>
    </div>
  </body>
</html>
